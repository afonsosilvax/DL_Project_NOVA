{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "n87BDsCnDh8q",
        "KUFubIKXkGOy",
        "si8l-mk2EEso",
        "A44Y4zFku2HD",
        "FhYs3iJhvAm3",
        "zd_2EiSSvhkK",
        "G-aqkam8EYmK",
        "ZbhiyrKkFCJd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0 - Dividing the dataset_rares_species into train,val,test (we run this once)"
      ],
      "metadata": {
        "id": "tpf-6vGSyqku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_cv\n",
        "!pip install keras_tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg967i9CKkGb",
        "outputId": "54efc107-5c7d-4e2b-9007-20fd88f2ca48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_cv\n",
            "  Downloading keras_cv-0.9.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras_cv) (24.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras_cv) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from keras_cv) (2024.11.6)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (from keras_cv) (4.9.8)\n",
            "Collecting keras-core (from keras_cv)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from keras_cv) (0.3.11)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras_cv) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras_cv) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras_cv) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (3.13.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from keras-core->keras_cv) (0.1.9)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (0.7.1)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (1.12.2)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (5.29.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (18.1.0)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (1.17.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (3.0.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras_cv) (1.17.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (4.13.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras_cv) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_cv) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_cv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_cv) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_cv) (2025.1.31)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->keras-core->keras_cv) (25.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from promise->tensorflow-datasets->keras_cv) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core->keras_cv) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core->keras_cv) (2.18.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow-datasets->keras_cv) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras_cv) (1.69.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras_cv) (0.1.2)\n",
            "Downloading keras_cv-0.9.0-py3-none-any.whl (650 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-core, keras_cv\n",
            "Successfully installed keras-core-0.1.7 keras_cv-0.9.0\n",
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras_tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras_tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras_tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras_tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras_tuner) (4.13.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras_tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras_tuner\n",
            "Successfully installed keras_tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_root = '/content/drive/MyDrive/Colab Notebooks/Deep Learning'\n",
        "dataset_dir = os.path.join(drive_root, \"dataset_rares_species\")  # Folder with raw images\n",
        "metadata_path = os.path.join(drive_root, \"species_metadata.csv\")  # Path to CSV\n",
        "output_dir = os.path.join(drive_root, \"data\")  # Where to save splits (train/val/test)\n",
        "\n",
        "# here we creaet the ouptut directories\n",
        "os.makedirs(os.path.join(output_dir, \"train\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, \"val\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, \"test\"), exist_ok=True)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4PnTeX7B7gU",
        "outputId": "e94a684a-ee14-42c6-f674-228db95860c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndrive.mount(\\'/content/drive\\')\\n\\ndrive_root = \\'/content/drive/MyDrive/Colab Notebooks/Deep Learning\\'\\ndataset_dir = os.path.join(drive_root, \"dataset_rares_species\")  # Folder with raw images\\nmetadata_path = os.path.join(drive_root, \"species_metadata.csv\")  # Path to CSV\\noutput_dir = os.path.join(drive_root, \"data\")  # Where to save splits (train/val/test)\\n\\n# here we creaet the ouptut directories\\nos.makedirs(os.path.join(output_dir, \"train\"), exist_ok=True)\\nos.makedirs(os.path.join(output_dir, \"val\"), exist_ok=True)\\nos.makedirs(os.path.join(output_dir, \"test\"), exist_ok=True)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "df = pd.read_csv(metadata_path)\n",
        "\n",
        "# holdout method\n",
        "train_val_df, test_df = train_test_split(df,test_size=0.15,stratify=df['family'],random_state=42)\n",
        "\n",
        "train_df, val_df = train_test_split(train_val_df,test_size=0.15/0.85,  stratify=train_val_df['family'],random_state=42)\n",
        "'''"
      ],
      "metadata": {
        "id": "uJAwfXb-CdNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d7fac9-8237-49dd-d456-67fdc1efe191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndf = pd.read_csv(metadata_path)\\n\\n# holdout method\\ntrain_val_df, test_df = train_test_split(df,test_size=0.15,stratify=df['family'],random_state=42)\\n\\ntrain_df, val_df = train_test_split(train_val_df,test_size=0.15/0.85,  stratify=train_val_df['family'],random_state=42)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def copy_images(split_df, split_name):\n",
        "    for _, row in tqdm(split_df.iterrows(), desc=f\"Copying {split_name} images\"):\n",
        "        # Source path: Parse from 'file_path' column\n",
        "        src_path = os.path.join(dataset_dir, row['file_path'])\n",
        "\n",
        "        dest_dir = os.path.join(output_dir, split_name, row['family'])\n",
        "        os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "        filename = os.path.basename(row['file_path'])\n",
        "        dest_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "        if os.path.exists(src_path):\n",
        "            copyfile(src_path, dest_path)\n",
        "        else:\n",
        "            print(f\"Warning: {src_path} not found\")\n",
        "\n",
        "copy_images(train_df, \"train\")\n",
        "copy_images(val_df, \"val\")\n",
        "copy_images(test_df, \"test\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr7HxX1lCqSP",
        "outputId": "9e2aa9dd-78a7-4fd9-809e-d43283a01967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef copy_images(split_df, split_name):\\n    for _, row in tqdm(split_df.iterrows(), desc=f\"Copying {split_name} images\"):\\n        # Source path: Parse from \\'file_path\\' column\\n        src_path = os.path.join(dataset_dir, row[\\'file_path\\'])\\n\\n        dest_dir = os.path.join(output_dir, split_name, row[\\'family\\'])\\n        os.makedirs(dest_dir, exist_ok=True)\\n\\n        filename = os.path.basename(row[\\'file_path\\'])\\n        dest_path = os.path.join(dest_dir, filename)\\n\\n        if os.path.exists(src_path):\\n            copyfile(src_path, dest_path)\\n        else:\\n            print(f\"Warning: {src_path} not found\")\\n\\ncopy_images(train_df, \"train\")\\ncopy_images(val_df, \"val\")\\ncopy_images(test_df, \"test\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copying train images: 464it [07:56,  1.19it/s]Warning: /content/drive/MyDrive/Colab Notebooks/Deep Learning/dataset_rares_species/arthropoda_apidae/28260824_1065329_eol-full-size-copy.jpg not found\n",
        "Copying train images: 5104it [1:12:41,  1.17it/s]Warning: /content/drive/MyDrive/Colab Notebooks/Deep Learning/dataset_rares_species/arthropoda_apidae/28214440_1065290_eol-full-size-copy.jpg not found\n",
        "Copying train images: 7311it [1:43:10,  1.19it/s]Warning: /content/drive/MyDrive/Colab Notebooks/Deep Learning/dataset_rares_species/arthropoda_apidae/28260831_1065329_eol-full-size-copy.jpg not found\n",
        "Copying train images: 7968it [1:52:16,  1.18it/s]Warning: /content/drive/MyDrive/Colab Notebooks/Deep Learning/dataset_rares_species/arthropoda_apidae/28214384_1065290_eol-full-size-copy.jpg not found\n",
        "Copying train images: 8387it [1:58:04,  1.18it/s]\n",
        "Copying val images: 626it [08:41,  1.31it/s]Warning: /content/drive/MyDrive/Colab Notebooks/Deep Learning/dataset_rares_species/arthropoda_apidae/22375122_1065346_eol-full-size-copy.jpg not found\n",
        "Copying val images: 1798it [24:38,  1.22it/s]\n",
        "Copying test images: 98it [01:21,  1.15it/s]Warning: /content/drive/MyDrive/Colab Notebooks/Deep Learning/dataset_rares_species/arthropoda_apidae/28408134_1065346_eol-full-size-copy.jpg not found\n",
        "Copying test images: 1798it [24:39,  1.22it/s]"
      ],
      "metadata": {
        "id": "2VaSoq-P_lZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0 - Creating a subset of the data do the tests (we run this once)"
      ],
      "metadata": {
        "id": "-KcfDiuQPBcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "random.seed(42)\n",
        "\n",
        "drive_root = '/content/drive/MyDrive/Colab Notebooks/Deep Learning'\n",
        "data_dir = os.path.join(drive_root, \"data\")\n",
        "exp_dir = os.path.join(drive_root, \"experimentation\")\n",
        "\n",
        "train_size = 1676\n",
        "val_size = 358\n",
        "test_size = 358\n",
        "\n",
        "os.makedirs(os.path.join(exp_dir, \"train\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(exp_dir, \"val\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(exp_dir, \"test\"), exist_ok=True)\n",
        "\n",
        "def create_subset(src_dir, dest_dir, target_size):\n",
        "    class_folders = [f for f in os.listdir(src_dir) if os.path.isdir(os.path.join(src_dir, f))]\n",
        "\n",
        "    total_images = 0\n",
        "    class_counts = {}\n",
        "    for cls in class_folders:\n",
        "        cls_path = os.path.join(src_dir, cls)\n",
        "        image_files = [f for f in os.listdir(cls_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        class_counts[cls] = len(image_files)\n",
        "        total_images += len(image_files)\n",
        "\n",
        "    class_samples = {}\n",
        "    remaining = target_size\n",
        "    for cls, count in class_counts.items():\n",
        "        samples = max(1, int(count * target_size / total_images))\n",
        "        class_samples[cls] = min(samples, count)\n",
        "        remaining -= class_samples[cls]\n",
        "\n",
        "    if remaining > 0:\n",
        "        classes_sorted = sorted(class_counts.keys(), key=lambda c: class_counts[c], reverse=True)\n",
        "        for cls in classes_sorted:\n",
        "            if class_samples[cls] < class_counts[cls]:\n",
        "                class_samples[cls] += 1\n",
        "                remaining -= 1\n",
        "            if remaining == 0:\n",
        "                break\n",
        "    elif remaining < 0:\n",
        "        classes_sorted = sorted(class_counts.keys(), key=lambda c: class_counts[c], reverse=True)\n",
        "        for cls in classes_sorted:\n",
        "            if class_samples[cls] > 1:\n",
        "                class_samples[cls] -= 1\n",
        "                remaining += 1\n",
        "            if remaining == 0:\n",
        "                break\n",
        "\n",
        "\n",
        "    for cls in tqdm(class_folders, desc=f\"Creating subset in {os.path.basename(dest_dir)}\"):\n",
        "        src_class_dir = os.path.join(src_dir, cls)\n",
        "        dest_class_dir = os.path.join(dest_dir, cls)\n",
        "        os.makedirs(dest_class_dir, exist_ok=True)\n",
        "        image_files = [f for f in os.listdir(src_class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "\n",
        "        samples_to_take = min(class_samples[cls], len(image_files))\n",
        "        sampled_files = random.sample(image_files, samples_to_take)\n",
        "\n",
        "        for file in sampled_files:\n",
        "            src_file = os.path.join(src_class_dir, file)\n",
        "            dest_file = os.path.join(dest_class_dir, file)\n",
        "            shutil.copy2(src_file, dest_file)\n",
        "\n",
        "    copied_total = 0\n",
        "    for cls in class_folders:\n",
        "        dest_class_dir = os.path.join(dest_dir, cls)\n",
        "        if os.path.exists(dest_class_dir):\n",
        "            copied_total += len(os.listdir(dest_class_dir))\n",
        "\n",
        "    print(f\"Copied {copied_total} images to {os.path.basename(dest_dir)}\")\n",
        "    return copied_total\n",
        "\n",
        "print(\"Creating experimentation dataset...\")\n",
        "train_copied = create_subset(\n",
        "    os.path.join(data_dir, \"train\"),\n",
        "    os.path.join(exp_dir, \"train\"),\n",
        "    train_size\n",
        ")\n",
        "\n",
        "val_copied = create_subset(\n",
        "    os.path.join(data_dir, \"val\"),\n",
        "    os.path.join(exp_dir, \"val\"),\n",
        "    val_size\n",
        ")\n",
        "\n",
        "test_copied = create_subset(\n",
        "    os.path.join(data_dir, \"test\"),\n",
        "    os.path.join(exp_dir, \"test\"),\n",
        "    test_size\n",
        ")\n",
        "\n",
        "print(f\"\\nExperimentation dataset created:\")\n",
        "print(f\"Train: {train_copied} images\")\n",
        "print(f\"Validation: {val_copied} images\")\n",
        "print(f\"Test: {test_copied} images\")\n",
        "print(f\"Total: {train_copied + val_copied + test_copied} images\")\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l50429WjO-oj",
        "outputId": "dd6efb93-40b9-4cbc-e78d-d5907865aa9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport os\\nimport random\\nimport shutil\\nfrom pathlib import Path\\nfrom tqdm import tqdm\\nrandom.seed(42)\\n\\ndrive_root = \\'/content/drive/MyDrive/Colab Notebooks/Deep Learning\\'\\ndata_dir = os.path.join(drive_root, \"data\")  \\nexp_dir = os.path.join(drive_root, \"experimentation\")  \\n\\ntrain_size = 1676\\nval_size = 358\\ntest_size = 358\\n\\nos.makedirs(os.path.join(exp_dir, \"train\"), exist_ok=True)\\nos.makedirs(os.path.join(exp_dir, \"val\"), exist_ok=True)\\nos.makedirs(os.path.join(exp_dir, \"test\"), exist_ok=True)\\n\\ndef create_subset(src_dir, dest_dir, target_size):\\n    class_folders = [f for f in os.listdir(src_dir) if os.path.isdir(os.path.join(src_dir, f))]\\n\\n    total_images = 0\\n    class_counts = {}\\n    for cls in class_folders:\\n        cls_path = os.path.join(src_dir, cls)\\n        image_files = [f for f in os.listdir(cls_path) if f.lower().endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\'))]\\n        class_counts[cls] = len(image_files)\\n        total_images += len(image_files)\\n\\n    class_samples = {}\\n    remaining = target_size\\n    for cls, count in class_counts.items():\\n        samples = max(1, int(count * target_size / total_images))\\n        class_samples[cls] = min(samples, count)  \\n        remaining -= class_samples[cls]\\n\\n    if remaining > 0:\\n        classes_sorted = sorted(class_counts.keys(), key=lambda c: class_counts[c], reverse=True)\\n        for cls in classes_sorted:\\n            if class_samples[cls] < class_counts[cls]:\\n                class_samples[cls] += 1\\n                remaining -= 1\\n            if remaining == 0:\\n                break\\n    elif remaining < 0:\\n        classes_sorted = sorted(class_counts.keys(), key=lambda c: class_counts[c], reverse=True)\\n        for cls in classes_sorted:\\n            if class_samples[cls] > 1:  \\n                class_samples[cls] -= 1\\n                remaining += 1\\n            if remaining == 0:\\n                break\\n\\n    \\n    for cls in tqdm(class_folders, desc=f\"Creating subset in {os.path.basename(dest_dir)}\"):\\n        src_class_dir = os.path.join(src_dir, cls)\\n        dest_class_dir = os.path.join(dest_dir, cls)\\n        os.makedirs(dest_class_dir, exist_ok=True)\\n        image_files = [f for f in os.listdir(src_class_dir) if f.lower().endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\'))]\\n\\n  \\n        samples_to_take = min(class_samples[cls], len(image_files))\\n        sampled_files = random.sample(image_files, samples_to_take)\\n\\n        for file in sampled_files:\\n            src_file = os.path.join(src_class_dir, file)\\n            dest_file = os.path.join(dest_class_dir, file)\\n            shutil.copy2(src_file, dest_file)\\n\\n    copied_total = 0\\n    for cls in class_folders:\\n        dest_class_dir = os.path.join(dest_dir, cls)\\n        if os.path.exists(dest_class_dir):\\n            copied_total += len(os.listdir(dest_class_dir))\\n\\n    print(f\"Copied {copied_total} images to {os.path.basename(dest_dir)}\")\\n    return copied_total\\n\\nprint(\"Creating experimentation dataset...\")\\ntrain_copied = create_subset(\\n    os.path.join(data_dir, \"train\"),\\n    os.path.join(exp_dir, \"train\"),\\n    train_size\\n)\\n\\nval_copied = create_subset(\\n    os.path.join(data_dir, \"val\"),\\n    os.path.join(exp_dir, \"val\"),\\n    val_size\\n)\\n\\ntest_copied = create_subset(\\n    os.path.join(data_dir, \"test\"),\\n    os.path.join(exp_dir, \"test\"),\\n    test_size\\n)\\n\\nprint(f\"\\nExperimentation dataset created:\")\\nprint(f\"Train: {train_copied} images\")\\nprint(f\"Validation: {val_copied} images\")\\nprint(f\"Test: {test_copied} images\")\\nprint(f\"Total: {train_copied + val_copied + test_copied} images\")\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating experimentation dataset...\n",
        "\n",
        "Creating subset in train: 100%|██████████| 202/202 [03:16<00:00,  1.03it/s]\n",
        "\n",
        "Copied 1676 images to train\n",
        "\n",
        "Creating subset in val: 100%|██████████| 202/202 [02:49<00:00,  1.19it/s]\n",
        "\n",
        "Copied 358 images to val\n",
        "\n",
        "Creating subset in test: 100%|██████████| 202/202 [02:48<00:00,  1.20it/s]\n",
        "\n",
        "Copied 358 images to test\n",
        "\n",
        "Experimentation dataset created:\n",
        "\n",
        "- Train: 1676 images\n",
        "\n",
        "- Validation: 358 images\n",
        "\n",
        "- Test: 358 images\n",
        "\n",
        "- Total: 2392 images\n"
      ],
      "metadata": {
        "id": "-FMCxBABSPP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup and Imports"
      ],
      "metadata": {
        "id": "i-CRuHDtwrI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Importing libraries"
      ],
      "metadata": {
        "id": "tEE_hCmAkAdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#some basic libraries\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "from shutil import copyfile\n",
        "from typing import Any, Dict, List, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# tensorflow e keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras\n",
        "import keras_tuner as kt\n",
        "from keras import Model, Sequential, layers\n",
        "from keras.applications import ResNet50,MobileNetV2\n",
        "\n",
        "from keras.layers import Dense,Dropout,Conv2D,MaxPooling2D,Flatten,GlobalAveragePooling2D,add,LayerNormalization,RandomFlip,RandomRotation,RandomBrightness,RandomContrast,RandomTranslation,RandomZoom, Pipeline\n",
        "from keras.optimizers import SGD\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from keras.metrics import Accuracy,Precision,Recall,AUC,F1Score,CategoricalAccuracy\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler,EarlyStopping\n",
        "\n",
        "from keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "import keras_cv\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_root = '/content/drive/MyDrive/Colab Notebooks/Deep Learning'\n",
        "metadata_path = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/species_metadata.csv'\n",
        "\n",
        "\n",
        "df = pd.read_csv(metadata_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "NOwNdSWMw7Hf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4002c69-17f5-4b61-bddd-113ed7092449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 - Dataset Loading and Preprocessing"
      ],
      "metadata": {
        "id": "YEiJl_sV2Wd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 202\n",
        "batch_size = 64\n",
        "input_shape = (224, 224, 3)\n",
        "image_size = (224, 224)\n",
        "value_range = (0.0, 1.0)"
      ],
      "metadata": {
        "id": "Xam19e1onvet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we select false below, we are using the full dataset and not the sample"
      ],
      "metadata": {
        "id": "A0ZzyYmPpTlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_SUBSET = False\n",
        "\n",
        "if USE_SUBSET:\n",
        "    data_source = os.path.join(drive_root, \"experimentation\")\n",
        "    print(\"Using EXPERIMENTAL SUBSET for faster training\")\n",
        "else:\n",
        "    data_source = os.path.join(drive_root, \"data\")\n",
        "    print(\"Using FULL DATASET for final training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvHURdkjnqkc",
        "outputId": "54ac5b39-dc15-4378-ea45-00076286834b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using FULL DATASET for final training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = image_dataset_from_directory(\n",
        "    os.path.join(data_source, \"train\"),\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,\n",
        "    interpolation=\"bilinear\",\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_ds = image_dataset_from_directory(\n",
        "    os.path.join(data_source, \"val\"),\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,\n",
        "    interpolation=\"bilinear\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_ds = image_dataset_from_directory(\n",
        "    os.path.join(data_source, \"test\"),\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,\n",
        "    interpolation=\"bilinear\",\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "vzlflXIc2ExZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a57c222-3696-423f-e094-aa359694b583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8383 files belonging to 202 classes.\n",
            "Found 1797 files belonging to 202 classes.\n",
            "Found 1797 files belonging to 202 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prefetching and Parallelizing data transformation"
      ],
      "metadata": {
        "id": "q6jtrMHuppDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source:https://www.tensorflow.org/guide/data_performance#prefetching"
      ],
      "metadata": {
        "id": "JipMu8Awrmu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Prefetching overlaps the preprocessing and model execution of a training step. While the model is executing training step s, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data.\n",
        "\n",
        "The tf.data API provides the tf.data.Dataset.prefetch transformation. It can be used to decouple the time when data is produced from the time when data is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. You could either manually tune this value, or set it to tf.data.AUTOTUNE, which will prompt the tf.data runtime to tune the value dynamically at runtime.\n",
        "\n",
        "Note that the prefetch transformation provides benefits any time there is an opportunity to overlap the work of a \"producer\" with the work of a \"consumer.\" \""
      ],
      "metadata": {
        "id": "ElBowESxrffI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "5j66pIUjr7cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "normalization_layer = layers.Rescaling(1./255)\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
        "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "Gy4xGzvBptLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Check and verify GPU availability"
      ],
      "metadata": {
        "id": "6oIoIJ1wzInc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://stackoverflow.com/questions/71952532/automatically-check-available-gpu-on-google-colab"
      ],
      "metadata": {
        "id": "asFfu-vetJTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DomgFb0FzMoC",
        "outputId": "906ad514-9834-4673-e4b9-794eb79bbdf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 16 21:34:32 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0             27W /   70W |     104MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Helper Classes and Functions"
      ],
      "metadata": {
        "id": "rv1UC5-Mz2Oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: pratical class 5, utils.py"
      ],
      "metadata": {
        "id": "5tpXGA8MzKps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exp_decay_lr_scheduler(\n",
        "    epoch: int,\n",
        "    current_lr: float,\n",
        "    factor: float = 0.95\n",
        ") -> float:\n",
        "    current_lr *= factor\n",
        "    return current_lr"
      ],
      "metadata": {
        "id": "udIJGt4D0DKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Defining Data Augmentation Strategies"
      ],
      "metadata": {
        "id": "yGoxkj7g6lLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to define 3 different augmentation pipelines with varying levels of complexity."
      ],
      "metadata": {
        "id": "1MJw0WaazcvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: pratical class 4"
      ],
      "metadata": {
        "id": "TzSXLIwqzm9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_augmentation = Pipeline(\n",
        "    [\n",
        "        RandomBrightness(factor=0.1, value_range=value_range),\n",
        "        RandomFlip(\"horizontal\"),\n",
        "        RandomRotation(factor=0.1, fill_mode=\"reflect\")\n",
        "    ],\n",
        "    name=\"simple_augmentation\"\n",
        ")\n",
        "\n",
        "medium_augmentation = Pipeline(\n",
        "    [\n",
        "        RandomBrightness(factor=0.15, value_range=value_range),\n",
        "        RandomFlip(\"horizontal\"),\n",
        "        RandomRotation(factor=0.15, fill_mode=\"reflect\"),\n",
        "        RandomContrast(factor=0.1, value_range=value_range),\n",
        "        RandomTranslation(height_factor=0.1, width_factor=0.1, fill_mode=\"reflect\")\n",
        "    ],\n",
        "    name=\"medium_augmentation\"\n",
        ")\n",
        "\n",
        "complex_augmentation = Pipeline(\n",
        "    [\n",
        "        RandomBrightness(factor=0.2, value_range=value_range),\n",
        "        RandomFlip(\"horizontal\"),\n",
        "        RandomFlip(\"vertical\"),\n",
        "        RandomRotation(factor=0.3, fill_mode=\"reflect\"),\n",
        "        RandomContrast(factor=0.2, value_range=value_range),\n",
        "        RandomTranslation(height_factor=0.15, width_factor=0.15, fill_mode=\"reflect\"),\n",
        "        RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2), fill_mode=\"reflect\"),\n",
        "\n",
        "    ],\n",
        "    name=\"complex_augmentation\"\n",
        ")"
      ],
      "metadata": {
        "id": "MJjTx6Lc6OA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function below selects the augmentation based on complexity level"
      ],
      "metadata": {
        "id": "YUgIYG6V0iba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_augmentation(complexity=\"medium\"):\n",
        "    if complexity.lower() == \"simple\":\n",
        "        return simple_augmentation\n",
        "    elif complexity.lower() == \"complex\":\n",
        "        return complex_augmentation\n",
        "    else:\n",
        "        return medium_augmentation"
      ],
      "metadata": {
        "id": "vF1a3s-90zke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Architecture"
      ],
      "metadata": {
        "id": "n87BDsCnDh8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### GlobalAveragePooling2D justification"
      ],
      "metadata": {
        "id": "yXgyvtygECIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our rare species classification project, we chose to use GlobalAveragePooling2D instead of Flatten for several compelling reasons:\n",
        "\n",
        "1. Parameter Efficiency: Using GlobalAveragePooling2D dramatically reduces model complexity. As noted by Saturn Cloud, \"If the final tensor shape before flattening is still large... Flatten will make an insane amount of parameters,\" while GlobalAveragePooling2D significantly condenses these parameters . This is crucial when dealing with rare species classification where we have 202 classes and limited training samples.\n",
        "\n",
        "2. Regularization Effect: GlobalAveragePooling2D serves as an inherent regularizer that helps prevent overfitting, which is particularly important for imbalanced datasets like ours. Stack Overflow discussions point out that GlobalAveragePooling2D is \"less prone to overfitting than a normal fully connected layer\" .\n",
        "\n",
        "3. Translation Invariance: For species classification, we care more about the presence of distinctive features than their exact location. GlobalAveragePooling2D preserves this translation invariance property by averaging across spatial dimensions . This makes our model more robust to variations in how species appear in different images.\n",
        "\n",
        "4. Modern Architecture Design: Most state-of-the-art CNN architectures like ResNet, DenseNet, and Inception use GlobalAveragePooling2D instead of Flatten. As noted by StackAbuse, \"Global Pooling condenses all of the feature maps into a single one, pooling all of the relevant information into a single map that can be easily understood by a single dense classification layer\" .\n",
        "\n",
        "5. Memory Efficiency: The computational advantage is significant - a Stack Overflow thread explains that for a tensor shape like (16, 240, 240, 128), Flatten would create 7,372,800 parameters, while GlobalAveragePooling2D produces just 128 . This efficiency was essential for our limited computational resources.\n",
        "\n",
        "Sources:\n",
        "1. Saturn Cloud Blog (2023): \"Understanding the Difference Between Flatten() and GlobalAveragePooling2D() in Keras\"\n",
        "https://saturncloud.io/blog/understanding-the-difference-between-flatten-and-globalaveragepooling2d-in-keras/\n",
        "\n",
        "2. Stack Overflow: \"What is the difference between Flatten() and GlobalAveragePooling2D() in keras\"\n",
        "https://stackoverflow.com/questions/49295311/what-is-the-difference-between-flatten-and-globalaveragepooling2d-in-keras\n",
        "3. StackAbuse (2023): \"Don't Use Flatten() - Global Pooling for CNNs with TensorFlow and Keras\"\n",
        "https://stackabuse.com/dont-use-flatten-global-pooling-for-cnns-with-tensorflow-and-keras/\n",
        "4. Data Science Stack Exchange: \"Is Flatten() layer in keras necessary?\"\n",
        "https://datascience.stackexchange.com/questions/80072/is-flatten-layer-in-keras-necessary\n",
        "5. Digital Ocean (2024): \"Global Pooling in Convolutional Neural Networks\"\n",
        "https://www.digitalocean.com/community/tutorials/global-pooling-in-convolutional-neural-networks"
      ],
      "metadata": {
        "id": "ttMi-fVkA5z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Frozing layers justification"
      ],
      "metadata": {
        "id": "ejj9bO3zEin-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://keras.io/guides/transfer_learning/"
      ],
      "metadata": {
        "id": "8tWWVZMsEm4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet50"
      ],
      "metadata": {
        "id": "A44Y4zFku2HD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet doesn't require dropout because it already implements batch normalization layers that provide sufficient regularization effects. Additionally, the skip connections in ResNet's architecture maintain a healthy gradient flow through the network, which helps prevent overfitting without the need for dropout. Research has consistently shown that adding dropout to ResNet architectures provides minimal benefits and can actually disrupt the benefits of the residual connections."
      ],
      "metadata": {
        "id": "gQ_zRcX8L1EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RareSpeciesResNet(Model):\n",
        "\n",
        "    def __init__(self, num_classes, unfrozen_layers=10, augmentation_type=\"medium\"):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.augmentation_layer = get_augmentation(augmentation_type)\n",
        "        self.pre_trained_architecture = ResNet50(include_top=False,weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "        # Freeze all layers except the last unfrozen_layers\n",
        "        for layer in self.pre_trained_architecture.layers[:-unfrozen_layers]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # Final layers for classification\n",
        "        self.global_pool_layer = GlobalAveragePooling2D()\n",
        "        self.dense_layer = Dense(self.num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "\n",
        "        # Apply augmentation only during training\n",
        "        if training:\n",
        "            x = self.augmentation_layer(inputs, training=training)\n",
        "        else:\n",
        "            x = inputs\n",
        "\n",
        "        x = self.pre_trained_architecture(x)\n",
        "        x = self.global_pool_layer(x)\n",
        "\n",
        "        return self.dense_layer(x)\n"
      ],
      "metadata": {
        "id": "0U1A7SVtvRyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MobileNetV2"
      ],
      "metadata": {
        "id": "FhYs3iJhvAm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MobileNet incorporates dropout layers because its lightweight architecture makes it more susceptible to overfitting. Unlike ResNet, MobileNet uses depthwise separable convolutions to reduce parameters, creating a trade-off between model capacity and regularization needs. The dropout layer in MobileNet helps prevent co-adaptation of features during training, which is especially important when working with limited training data like in our rare species classification task."
      ],
      "metadata": {
        "id": "LpunmR7oOqcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RareSpeciesMobileNet(Model):\n",
        "\n",
        "    def __init__(self, num_classes, unfrozen_layers=15, augmentation_type=\"medium\"):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.augmentation_layer = get_augmentation(augmentation_type)\n",
        "        self.pre_trained_architecture = MobileNetV2(include_top=False, weights='imagenet', input_shape= input_shape)\n",
        "\n",
        "        # Freeze all layers except the last unfrozen_layers\n",
        "        for layer in self.pre_trained_architecture.layers[:-unfrozen_layers]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # Final layers for classification\n",
        "        self.global_pool_layer = GlobalAveragePooling2D()\n",
        "        self.dropout_layer = Dropout(0.2)\n",
        "        self.dense_layer = Dense(self.num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "\n",
        "        if training:\n",
        "            x = self.augmentation_layer(inputs, training=training)\n",
        "        else:\n",
        "            x = inputs\n",
        "\n",
        "        x = self.pre_trained_architecture(x)\n",
        "        x = self.global_pool_layer(x)\n",
        "        x = self.dropout_layer(x, training=training)\n",
        "\n",
        "        return self.dense_layer(x)"
      ],
      "metadata": {
        "id": "tD48T_SuvK-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vit Model"
      ],
      "metadata": {
        "id": "zd_2EiSSvhkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- google/vit-base-patch16-224 is the official Google model hosted on Hugging Face, implemented in PyTorch and originally trained by Google Research. It's pretrained on ImageNet-21k and fine-tuned on ImageNet-1k at 224x224 resolution.\n",
        "- Huggingface\n",
        "sayakpaul/vit_b16_fe is a TensorFlow implementation by Sayak Paul, specifically designed for feature extraction . It's part of Sayak Paul's collection of TensorFlow Hub models converted from the original JAX implementation."
      ],
      "metadata": {
        "id": "ZghNkLg3SfJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used sayakpaul/vit_b16_fe. It is the better choice since it's specifically designed for feature extraction without requiring modifications to remove the classification head, and it aligns well with our transfer learning approach that requires high-quality image feature representations."
      ],
      "metadata": {
        "id": "onYfG-qMS4V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RareSpeciesViT(Model):\n",
        "\n",
        "    def __init__(self, num_classes, unfrozen_layers=2, augmentation_type=\"simple\"):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.augmentation_layer = get_augmentation(augmentation_type)\n",
        "\n",
        "        self.pre_trained_architecture = hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_b16_fe/1\",trainable=(unfrozen_layers > 0))\n",
        "\n",
        "        # Final layer for classification\n",
        "        self.dense_layer = Dense(self.num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "\n",
        "        if training:\n",
        "            x = self.augmentation_layer(inputs, training=training)\n",
        "        else:\n",
        "            x = inputs\n",
        "\n",
        "        x = self.pre_trained_architecture(x)\n",
        "\n",
        "        return self.dense_layer(x)"
      ],
      "metadata": {
        "id": "omZjUjtTvlE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Model Training Function with random search"
      ],
      "metadata": {
        "id": "G-aqkam8EYmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: based on practical class 5"
      ],
      "metadata": {
        "id": "YZ7zK2Y2mM4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics\n",
        "categorical_accuracy = CategoricalAccuracy(name=\"accuracy\")\n",
        "auc = AUC(name=\"auc\")\n",
        "f1_score = F1Score(average=\"macro\", name=\"f1_score\")\n",
        "metrics = [categorical_accuracy, auc, f1_score]"
      ],
      "metadata": {
        "id": "V6mg3ZJGkjMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = CategoricalCrossentropy(name=\"loss\")"
      ],
      "metadata": {
        "id": "cUw8NukOksEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "\n",
        "    model_type = hp.Choice('model_type', ['resnet'])\n",
        "\n",
        "    learning_rate = hp.Choice('learning_rate', [0.0001, 0.0003, 0.0005, 0.001, 0.003, 0.005, 0.01])\n",
        "    momentum = hp.Choice('momentum', [0.8, 0.85, 0.9, 0.95, 0.99])\n",
        "    weight_decay = hp.Choice('weight_decay', [1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3])\n",
        "    unfrozen_layers = hp.Choice('unfrozen_layers', [0, 5, 10, 15, 20])\n",
        "    augmentation_type = hp.Choice('augmentation_type', ['simple', 'medium', 'complex'])\n",
        "\n",
        "    if model_type == 'resnet':\n",
        "        model = RareSpeciesResNet(\n",
        "            num_classes,\n",
        "            unfrozen_layers=unfrozen_layers,\n",
        "            augmentation_type=augmentation_type\n",
        "        )\n",
        "    elif model_type == 'mobilenet':\n",
        "        model = RareSpeciesMobileNet(\n",
        "            num_classes,\n",
        "            unfrozen_layers=unfrozen_layers,\n",
        "            augmentation_type=augmentation_type\n",
        "        )\n",
        "    elif model_type == 'vit':\n",
        "        model = RareSpeciesViT(\n",
        "            num_classes,\n",
        "            unfrozen_layers=unfrozen_layers > 0,\n",
        "            augmentation_type=augmentation_type\n",
        "        )\n",
        "\n",
        "\n",
        "    optimizer = SGD(learning_rate=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics, run_eagerly=False)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "7EkQJipreP7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#definimos os diretorios aqui\n",
        "root_dir = Path(\"keras_tuner_results\")\n",
        "checkpoint_path = root_dir / \"checkpoint.keras\"\n",
        "metrics_path = root_dir / \"metrics.csv\""
      ],
      "metadata": {
        "id": "gTNFJX-8lk7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adaptado das aulas\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "metrics_callback = CSVLogger(metrics_path)\n",
        "lr_scheduler_callback = LearningRateScheduler(exp_decay_lr_scheduler)\n",
        "early_callback= EarlyStopping(monitor=\"val_loss\",patience=10,restore_best_weights=True,verbose=1)\n",
        "\n",
        "callbacks = [checkpoint_callback, metrics_callback, lr_scheduler_callback,early_callback]"
      ],
      "metadata": {
        "id": "Z6D8xiIgmHOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model Training and Evaluation"
      ],
      "metadata": {
        "id": "uzCTx-3aEsku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuner setup\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=3,\n",
        "    executions_per_trial=1,\n",
        "    directory=str(root_dir),\n",
        "    project_name=\"rare_species_tuning\"\n",
        ")\n",
        "\n",
        "# Start tuning\n",
        "tuner.search(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=200,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb2qM3UroH7c",
        "outputId": "2bcc57f7-dcc9-4c45-b925-26d6c17c0962"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 1 Complete [02h 16m 50s]\n",
            "val_accuracy: 0.6605453491210938\n",
            "\n",
            "Best val_accuracy So Far: 0.6605453491210938\n",
            "Total elapsed time: 02h 16m 50s\n",
            "\n",
            "Search: Running Trial #2\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "resnet            |resnet            |model_type\n",
            "0.01              |0.0005            |learning_rate\n",
            "0.85              |0.99              |momentum\n",
            "0.0003            |0.001             |weight_decay\n",
            "5                 |0                 |unfrozen_layers\n",
            "simple            |simple            |augmentation_type\n",
            "\n",
            "Epoch 1/200\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 5.05745, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 189s - 1s/step - accuracy: 0.1439 - auc: 0.7136 - f1_score: 0.1757 - loss: 5.0963 - val_accuracy: 0.0479 - val_auc: 0.6576 - val_f1_score: 0.0017 - val_loss: 5.0574 - learning_rate: 0.0095\n",
            "Epoch 2/200\n",
            "\n",
            "Epoch 2: val_loss improved from 5.05745 to 4.98034, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 196s - 1s/step - accuracy: 0.0426 - auc: 0.6821 - f1_score: 0.0067 - loss: 4.9495 - val_accuracy: 0.0456 - val_auc: 0.6806 - val_f1_score: 0.0077 - val_loss: 4.9803 - learning_rate: 0.0090\n",
            "Epoch 3/200\n",
            "\n",
            "Epoch 3: val_loss improved from 4.98034 to 4.93874, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 178s - 1s/step - accuracy: 0.0526 - auc: 0.6911 - f1_score: 0.0096 - loss: 4.8795 - val_accuracy: 0.0456 - val_auc: 0.6875 - val_f1_score: 0.0050 - val_loss: 4.9387 - learning_rate: 0.0086\n",
            "Epoch 4/200\n",
            "\n",
            "Epoch 4: val_loss improved from 4.93874 to 4.87897, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 201s - 2s/step - accuracy: 0.0623 - auc: 0.7051 - f1_score: 0.0127 - loss: 4.8230 - val_accuracy: 0.0495 - val_auc: 0.7036 - val_f1_score: 0.0057 - val_loss: 4.8790 - learning_rate: 0.0081\n",
            "Epoch 5/200\n",
            "\n",
            "Epoch 5: val_loss improved from 4.87897 to 4.87312, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 187s - 1s/step - accuracy: 0.0666 - auc: 0.7126 - f1_score: 0.0139 - loss: 4.7747 - val_accuracy: 0.0467 - val_auc: 0.6985 - val_f1_score: 0.0078 - val_loss: 4.8731 - learning_rate: 0.0077\n",
            "Epoch 6/200\n",
            "\n",
            "Epoch 6: val_loss improved from 4.87312 to 4.82346, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 193s - 1s/step - accuracy: 0.0763 - auc: 0.7232 - f1_score: 0.0187 - loss: 4.7220 - val_accuracy: 0.0518 - val_auc: 0.7051 - val_f1_score: 0.0090 - val_loss: 4.8235 - learning_rate: 0.0074\n",
            "Epoch 7/200\n",
            "\n",
            "Epoch 7: val_loss did not improve from 4.82346\n",
            "131/131 - 205s - 2s/step - accuracy: 0.0804 - auc: 0.7271 - f1_score: 0.0220 - loss: 4.6806 - val_accuracy: 0.0456 - val_auc: 0.7027 - val_f1_score: 0.0105 - val_loss: 4.8401 - learning_rate: 0.0070\n",
            "Epoch 8/200\n",
            "\n",
            "Epoch 8: val_loss improved from 4.82346 to 4.77568, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 184s - 1s/step - accuracy: 0.0809 - auc: 0.7339 - f1_score: 0.0204 - loss: 4.6529 - val_accuracy: 0.0646 - val_auc: 0.7166 - val_f1_score: 0.0126 - val_loss: 4.7757 - learning_rate: 0.0066\n",
            "Epoch 9/200\n",
            "\n",
            "Epoch 9: val_loss did not improve from 4.77568\n",
            "131/131 - 181s - 1s/step - accuracy: 0.0922 - auc: 0.7396 - f1_score: 0.0268 - loss: 4.6161 - val_accuracy: 0.0501 - val_auc: 0.7129 - val_f1_score: 0.0125 - val_loss: 4.7977 - learning_rate: 0.0063\n",
            "Epoch 10/200\n",
            "\n",
            "Epoch 10: val_loss improved from 4.77568 to 4.73694, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 191s - 1s/step - accuracy: 0.0952 - auc: 0.7465 - f1_score: 0.0273 - loss: 4.5833 - val_accuracy: 0.0723 - val_auc: 0.7288 - val_f1_score: 0.0153 - val_loss: 4.7369 - learning_rate: 0.0060\n",
            "Epoch 11/200\n",
            "\n",
            "Epoch 11: val_loss did not improve from 4.73694\n",
            "131/131 - 190s - 1s/step - accuracy: 0.0971 - auc: 0.7520 - f1_score: 0.0282 - loss: 4.5488 - val_accuracy: 0.0735 - val_auc: 0.7217 - val_f1_score: 0.0179 - val_loss: 4.7546 - learning_rate: 0.0057\n",
            "Epoch 12/200\n",
            "\n",
            "Epoch 12: val_loss improved from 4.73694 to 4.71429, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 175s - 1s/step - accuracy: 0.1021 - auc: 0.7556 - f1_score: 0.0323 - loss: 4.5231 - val_accuracy: 0.0846 - val_auc: 0.7223 - val_f1_score: 0.0212 - val_loss: 4.7143 - learning_rate: 0.0054\n",
            "Epoch 13/200\n",
            "\n",
            "Epoch 13: val_loss improved from 4.71429 to 4.68395, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 187s - 1s/step - accuracy: 0.1026 - auc: 0.7584 - f1_score: 0.0333 - loss: 4.5016 - val_accuracy: 0.0790 - val_auc: 0.7363 - val_f1_score: 0.0230 - val_loss: 4.6840 - learning_rate: 0.0051\n",
            "Epoch 14/200\n",
            "\n",
            "Epoch 14: val_loss improved from 4.68395 to 4.68242, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 195s - 1s/step - accuracy: 0.1072 - auc: 0.7654 - f1_score: 0.0357 - loss: 4.4784 - val_accuracy: 0.0824 - val_auc: 0.7336 - val_f1_score: 0.0221 - val_loss: 4.6824 - learning_rate: 0.0049\n",
            "Epoch 15/200\n",
            "\n",
            "Epoch 15: val_loss improved from 4.68242 to 4.64655, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 180s - 1s/step - accuracy: 0.1089 - auc: 0.7721 - f1_score: 0.0337 - loss: 4.4442 - val_accuracy: 0.0718 - val_auc: 0.7415 - val_f1_score: 0.0237 - val_loss: 4.6465 - learning_rate: 0.0046\n",
            "Epoch 16/200\n",
            "\n",
            "Epoch 16: val_loss improved from 4.64655 to 4.63495, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 199s - 2s/step - accuracy: 0.1093 - auc: 0.7737 - f1_score: 0.0328 - loss: 4.4270 - val_accuracy: 0.1013 - val_auc: 0.7402 - val_f1_score: 0.0312 - val_loss: 4.6350 - learning_rate: 0.0044\n",
            "Epoch 17/200\n",
            "\n",
            "Epoch 17: val_loss improved from 4.63495 to 4.62268, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 199s - 2s/step - accuracy: 0.1140 - auc: 0.7772 - f1_score: 0.0377 - loss: 4.4055 - val_accuracy: 0.0885 - val_auc: 0.7472 - val_f1_score: 0.0308 - val_loss: 4.6227 - learning_rate: 0.0042\n",
            "Epoch 18/200\n",
            "\n",
            "Epoch 18: val_loss did not improve from 4.62268\n",
            "131/131 - 211s - 2s/step - accuracy: 0.1202 - auc: 0.7847 - f1_score: 0.0431 - loss: 4.3748 - val_accuracy: 0.0846 - val_auc: 0.7416 - val_f1_score: 0.0284 - val_loss: 4.6333 - learning_rate: 0.0040\n",
            "Epoch 19/200\n",
            "\n",
            "Epoch 19: val_loss improved from 4.62268 to 4.57795, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 186s - 1s/step - accuracy: 0.1214 - auc: 0.7857 - f1_score: 0.0443 - loss: 4.3626 - val_accuracy: 0.0974 - val_auc: 0.7558 - val_f1_score: 0.0368 - val_loss: 4.5779 - learning_rate: 0.0038\n",
            "Epoch 20/200\n",
            "\n",
            "Epoch 20: val_loss did not improve from 4.57795\n",
            "131/131 - 182s - 1s/step - accuracy: 0.1204 - auc: 0.7901 - f1_score: 0.0446 - loss: 4.3425 - val_accuracy: 0.1007 - val_auc: 0.7488 - val_f1_score: 0.0324 - val_loss: 4.6006 - learning_rate: 0.0036\n",
            "Epoch 21/200\n",
            "\n",
            "Epoch 21: val_loss improved from 4.57795 to 4.55461, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 194s - 1s/step - accuracy: 0.1266 - auc: 0.7930 - f1_score: 0.0483 - loss: 4.3204 - val_accuracy: 0.1091 - val_auc: 0.7521 - val_f1_score: 0.0402 - val_loss: 4.5546 - learning_rate: 0.0034\n",
            "Epoch 22/200\n",
            "\n",
            "Epoch 22: val_loss improved from 4.55461 to 4.52951, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 213s - 2s/step - accuracy: 0.1230 - auc: 0.7959 - f1_score: 0.0443 - loss: 4.2982 - val_accuracy: 0.1074 - val_auc: 0.7592 - val_f1_score: 0.0398 - val_loss: 4.5295 - learning_rate: 0.0032\n",
            "Epoch 23/200\n",
            "\n",
            "Epoch 23: val_loss did not improve from 4.52951\n",
            "131/131 - 186s - 1s/step - accuracy: 0.1286 - auc: 0.7981 - f1_score: 0.0463 - loss: 4.2866 - val_accuracy: 0.1096 - val_auc: 0.7552 - val_f1_score: 0.0397 - val_loss: 4.5662 - learning_rate: 0.0031\n",
            "Epoch 24/200\n",
            "\n",
            "Epoch 24: val_loss did not improve from 4.52951\n",
            "131/131 - 173s - 1s/step - accuracy: 0.1315 - auc: 0.7976 - f1_score: 0.0501 - loss: 4.2775 - val_accuracy: 0.1157 - val_auc: 0.7581 - val_f1_score: 0.0439 - val_loss: 4.5502 - learning_rate: 0.0029\n",
            "Epoch 25/200\n",
            "\n",
            "Epoch 25: val_loss did not improve from 4.52951\n",
            "131/131 - 173s - 1s/step - accuracy: 0.1350 - auc: 0.8019 - f1_score: 0.0501 - loss: 4.2598 - val_accuracy: 0.1029 - val_auc: 0.7630 - val_f1_score: 0.0410 - val_loss: 4.5427 - learning_rate: 0.0028\n",
            "Epoch 26/200\n",
            "\n",
            "Epoch 26: val_loss improved from 4.52951 to 4.49236, saving model to keras_tuner_results/checkpoint.keras\n",
            "131/131 - 204s - 2s/step - accuracy: 0.1330 - auc: 0.8046 - f1_score: 0.0547 - loss: 4.2433 - val_accuracy: 0.1174 - val_auc: 0.7675 - val_f1_score: 0.0491 - val_loss: 4.4924 - learning_rate: 0.0026\n",
            "Epoch 27/200\n",
            "\n",
            "Epoch 27: val_loss did not improve from 4.49236\n",
            "131/131 - 211s - 2s/step - accuracy: 0.1375 - auc: 0.8075 - f1_score: 0.0536 - loss: 4.2193 - val_accuracy: 0.1119 - val_auc: 0.7609 - val_f1_score: 0.0397 - val_loss: 4.5449 - learning_rate: 0.0025\n",
            "Epoch 28/200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Melhor modelo\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Hiperparâmetros ideais\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Melhores hiperparâmetros encontrados:\")\n",
        "for k, v in best_hp.values.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "# Avaliação final no test set\n",
        "results = best_model.evaluate(test_ds, return_dict=True)\n",
        "print(\"Resultados no conjunto de teste:\", results)\n"
      ],
      "metadata": {
        "id": "ICN8h00poWD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Results"
      ],
      "metadata": {
        "id": "vPi0N3HxXeaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 Using the sample dataset with random search"
      ],
      "metadata": {
        "id": "2rdfjV9dX8OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1.1 MobileNet Trials"
      ],
      "metadata": {
        "id": "V3Swc9a-q4nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Trial | Learning Rate | Momentum | Weight Decay | Unfrozen Layers | Epochs | Patience | Augmentation     | Batch Size |\n",
        "|-------|---------------|----------|---------------|------------------|--------|----------|------------------|-------------|\n",
        "| 2     | 0.0005        | 0.99     | 1e-05         | 10               | 200    | 5        | complex          | 64          |\n",
        "| 3     | 0.003         | 0.85     | 0.0001        | 5                | 200    | 8        | simple           | 64          |\n",
        "| 4     | 0.0001        | 0.8      | 0.0001        | 20               | 200    | 5        | simple           | 64          |\n",
        "| 5     | 0.001         | 0.9      | 3e-06         | 0                | 200    | 8        | simple           | 64          |\n"
      ],
      "metadata": {
        "id": "TETPGfbyqrIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Trial | Val Accuracy | Train Accuracy | Test Accuracy | Val F1 | Train F1 | Test F1 | Val Loss | Train Loss | Test Loss | Overfitting | Time (s) | Notes         |\n",
        "|-------|--------------|----------------|---------------|--------|----------|---------|----------|-------------|------------|--------------|----------|---------------|\n",
        "| 2     | 0.0810       | 0.1128         | 0.1034        | 0.0087 | 0.0167   | 0.0109  | 5.0608   | 4.8286      | 5.0316     | 0.0318       | 336      | ❌ Very poor   |\n",
        "| 3     | 0.4106       | 0.9863         | 0.4022        | 0.3069 | 0.9902   | 0.2552  | 2.6863   | 0.2147      | 2.6404     | 0.5757       | 2216     | ✅ Best so far |\n",
        "| 4     | 0.3240       | 0.6981         | 0.3715        | 0.1651 | 0.5558   | 0.1987  | 3.2330   | 1.6887      | 3.2300     | 0.3741       | 9371     |               |\n",
        "| 5     | 0.3520       | 0.8001         | 0.3575        | 0.2385 | 0.7790   | 0.2125  | 3.1751   | 1.4093      | 3.1163     | 0.4482       | 1557     |               |\n"
      ],
      "metadata": {
        "id": "mLRG61M0qy9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1.2 VIT Trials"
      ],
      "metadata": {
        "id": "iW4X-BBgrEsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Trial | Learning Rate | Momentum | Weight Decay | Unfrozen Layers | Epochs | Patience | Augmentation | Batch Size |\n",
        "|-------|---------------|----------|--------------|------------------|--------|----------|--------------|------------|\n",
        "| 1     | 0.0003        | 0.95     | 3e-06        | 20               | 200    | 10       | simple       | 64         |\n",
        "| 2     | 0.005         | 0.99     | 1e-05        | 10               | 200    | 10       | medium       | 64         |\n",
        "| 3     | 0.003         | 0.8      | 0.0003       | 15               | 200    | 10       | simple       | 64         |\n",
        "| 4     | 0.0003        | 0.8      | 0.0001       | 0                | 200    | 10       | complex      | 64         |\n",
        "| 5     | 0.0005        | 0.85     | 3e-05        | 5                | 200    | 10       | simple       | 64         |\n"
      ],
      "metadata": {
        "id": "xdCHU5Y4q8YQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Trial | Val Accuracy | Train Accuracy | Test Accuracy | Val F1 | Train F1 | Test F1 | Val Loss | Train Loss | Test Loss | Overfitting | Time (s) | Notes         |\n",
        "|-------|--------------|----------------|---------------|--------|----------|---------|----------|-------------|------------|--------------|----------|---------------|\n",
        "| 1     | 0.6453       | 0.9916         | 0.6676        | 0.5281 | 0.9925   | 0.5578  | 1.5820   | 0.0709      | 1.3199     | 0.3464       | 10176    | ✅ Best so far |\n",
        "| 2     | 0.6313       | 0.9111         | 0.6341        | 0.5273 | 0.9028   | 0.5412  | 2.5510   | 0.3224      | 2.6848     | 0.2798       | 755      |               |\n",
        "| 3     | 0.6676       | 0.9958         | 0.6955        | 0.5596 | 0.9946   | 0.5851  | 1.5276   | 0.0399      | 1.2526     | 0.3282       | 8343     | ✅ New best    |\n",
        "| 4     | 0.5307       | 0.8067         | 0.5978        | 0.4073 | 0.7590   | 0.4576  | 2.0679   | 0.7848      | 1.7417     | 0.2760       | 11586    |               |\n",
        "| 5     | 0.6257       | 0.9869         | 0.6732        | 0.5232 | 0.9887   | 0.5587  | 1.5687   | 0.1265      | 1.4002     | 0.3612       | 13102    |               |\n"
      ],
      "metadata": {
        "id": "qCgu_jLUrBSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1.3 Resnet Trials"
      ],
      "metadata": {
        "id": "wIA5yqpaf36y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We started to do some trials, however, we dind't end up even the first one. ResNet-50 was ~7× slower than ViT-B16, and over 3× slower than MobileNetV2.\n",
        "\n",
        "Completing 200 epochs in a full dataset would have taken 55 hours with ResNet — practically infeasible under time and compute constraints.\n",
        "\n",
        "ResNet’s layer-by-layer structure with small kernel sizes and frequent branching makes it less GPU-friendly.\n",
        "\n",
        "It results in many small matrix multiplications, which underutilize GPU cores.\n",
        "\n",
        "ViT and MobileNetV2 have larger matrix ops or fewer, more parallelizable operations, making better use of modern GPUs.\n",
        "\n"
      ],
      "metadata": {
        "id": "zg1yjPUDjwCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 Running the best model of vit and mobilenet with original and full dataset"
      ],
      "metadata": {
        "id": "Czh1cR4sYHCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "| Trial | Model      | Learning Rate | Momentum | Weight Decay | Unfrozen Layers | Epochs | Patience | Augmentation | Batch Size |\n",
        "|-------|------------|----------------|----------|--------------|------------------|--------|----------|--------------|------------|\n",
        "| 3     | ViT        | 0.003          | 0.8      | 0.0003       | 15               | 200    | 10       | simple       | 64         |\n",
        "| 4     | MobileNet  | 0.0001         | 0.8      | 0.0001       | 20               | 200    | 5        | simple       | 64         |\n"
      ],
      "metadata": {
        "id": "bN5oQ7sRgSBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "| Metric                         | ViT           | MobileNet      | Difference       |\n",
        "|--------------------------------|---------------|----------------|------------------|\n",
        "| ✅ Training Accuracy           | 0.9888        | 0.8114         | +0.1774          |\n",
        "| ✅ Validation Accuracy         | 0.8141        | 0.5771         | +0.2370          |\n",
        "| ✅ Test Accuracy               | 0.8175        | 0.5782         | +0.2393          |\n",
        "| 🎯 Training F1 Score           | 0.9909        | 0.7957         | +0.1952          |\n",
        "| 🎯 Validation F1 Score         | 0.7961        | 0.5273         | +0.2688          |\n",
        "| 🎯 Test F1 Score               | 0.7987        | 0.5033         | +0.2954          |\n",
        "| ❌ Training Loss               | 0.0686        | 0.8947         | -0.8261          |\n",
        "| ❌ Validation Loss             | 0.7718        | 1.7799         | -1.0081          |\n",
        "| ❌ Test Loss                   | 0.7007        | 1.7437         | -1.0430          |\n",
        "| ⚠️ Overfitting (Train - Val Acc) | 0.1747        | 0.2343         | -0.0596          |\n",
        "| ⏱️ Training Time (s)          | 22472.98      | 42376.63       | -19903.65        |\n"
      ],
      "metadata": {
        "id": "1KAzEWblf75D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 Running the best model of vit with cleaned dataset without sintetic images"
      ],
      "metadata": {
        "id": "nbIT1wr8YirH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same configuration - trial 3\n",
        "\n",
        "\n",
        "\n",
        "| Metric                         | Cleaned Dataset | Original Dataset | Difference       |\n",
        "|--------------------------------|-----------------|------------------|------------------|\n",
        "| ✅ Training Accuracy           | 0.9888          | 0.9948           | -0.0060          |\n",
        "| ✅ Validation Accuracy         | 0.8141          | 0.8273           | -0.0132          |\n",
        "| ✅ Test Accuracy               | 0.8175          | 0.8264           | -0.0089          |\n",
        "| 🎯 Training F1 Score           | 0.9909          | 0.9963           | -0.0054          |\n",
        "| 🎯 Validation F1 Score         | 0.7961          | 0.8107           | -0.0146          |\n",
        "| 🎯 Test F1 Score               | 0.7987          | 0.8091           | -0.0104          |\n",
        "| ❌ Training Loss               | 0.0686          | 0.0450           | +0.0236          |\n",
        "| ❌ Validation Loss             | 0.7718          | 0.7366           | +0.0352          |\n",
        "| ❌ Test Loss                   | 0.7007          | 0.6871           | +0.0136          |\n",
        "| ⚠️ Overfitting (Train - Val Acc) | 0.1747          | 0.1675           | +0.0072          |\n",
        "| ⏱️ Training Time (s)          | 22472.98        | 24535.89         | -2052.91         |\n",
        "\n"
      ],
      "metadata": {
        "id": "VyGOW649eM9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 Running the best model of vit with cleaned dataset with sintetic images (64 x64)\n"
      ],
      "metadata": {
        "id": "AGRXK2DjSspQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "exp_decay_lr_scheduler - factor = 0.95\n",
        "\n",
        "| **Metric**           | **Training** | **Validation** |\n",
        "|-----------------------|------------|---------------|\n",
        "| **Accuracy**          | 0.7744     | 0.6040        |\n",
        "| **AUC**               | 0.9896     | 0.9575        |\n",
        "| **F1 Score**          | 0.7244     | 0.5418        |\n",
        "| **Loss**              | 0.9686     | 1.6317        |\n"
      ],
      "metadata": {
        "id": "wfhTj_1VSxCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.5 Running the best model of vit with cleaned dataset with sintetic images (224x224)"
      ],
      "metadata": {
        "id": "vmmTKI8BS4DI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric        | Training  | Validation |\n",
        "|---------------|:---------:|:----------:|\n",
        "| Accuracy      | 0.7846     | 0.6196     |\n",
        "| AUC           | 0.9857     | 0.9405     |\n",
        "| F1 Score      | 0.7351     | 0.5670     |\n",
        "| Loss          | 0.9939     | 1.7267     |"
      ],
      "metadata": {
        "id": "q6376HBbS8bp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.6 Running the best model of vit with cleaned dataset through DBSCAN"
      ],
      "metadata": {
        "id": "ZHZ7_zrFS_fd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results were very similar of the trial that we did of vit with cleaned dataset without sintetic images."
      ],
      "metadata": {
        "id": "ghmVZM1YTFNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.7 Running the best model of vit with cleaned dataset with sintetic and original images in the size of (224x224) with augmentation"
      ],
      "metadata": {
        "id": "sqbbPlpmTCoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric           | Training | Validation |\n",
        "|------------------|----------------|------------------|\n",
        "| Accuracy         | 0.9731         | 0.8524           |\n",
        "| AUC              | 0.9999         | 0.9892           |\n",
        "| F1 Score         | 0.9751         | 0.8559           |\n",
        "| Loss             | 0.1633         | 0.5866           |\n",
        "\n"
      ],
      "metadata": {
        "id": "wxkAoU7UTqA8"
      }
    }
  ]
}