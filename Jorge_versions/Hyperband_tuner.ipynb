{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "n87BDsCnDh8q",
        "A44Y4zFku2HD",
        "FhYs3iJhvAm3",
        "zd_2EiSSvhkK",
        "G-aqkam8EYmK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0 - Dividing the dataset_rares_species into train,val,test (we run this once)"
      ],
      "metadata": {
        "id": "tpf-6vGSyqku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "drive_root = '/content/drive/MyDrive/Colab Notebooks/Deep Learning'\n",
        "dataset_dir = os.path.join(drive_root, \"dataset_rares_species\")  # Folder with raw images\n",
        "metadata_path = os.path.join(drive_root, \"species_metadata.csv\")  # Path to CSV\n",
        "output_dir = os.path.join(drive_root, \"data\")  # Where to save splits (train/val/test)\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(os.path.join(output_dir, \"train\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, \"val\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, \"test\"), exist_ok=True)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "W4PnTeX7B7gU",
        "outputId": "c1e5a44f-0ea4-480f-9eb2-e1494e1a282e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# Mount Google Drive\\ndrive.mount(\\'/content/drive\\')\\n\\n# Define paths\\ndrive_root = \\'/content/drive/MyDrive/Colab Notebooks/Deep Learning\\'\\ndataset_dir = os.path.join(drive_root, \"dataset_rares_species\")  # Folder with raw images\\nmetadata_path = os.path.join(drive_root, \"species_metadata.csv\")  # Path to CSV\\noutput_dir = os.path.join(drive_root, \"data\")  # Where to save splits (train/val/test)\\n\\n# Create output directories\\nos.makedirs(os.path.join(output_dir, \"train\"), exist_ok=True)\\nos.makedirs(os.path.join(output_dir, \"val\"), exist_ok=True)\\nos.makedirs(os.path.join(output_dir, \"test\"), exist_ok=True)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Load CSV\n",
        "df = pd.read_csv(metadata_path)\n",
        "\n",
        "# Split into train/val/test (stratified by family)\n",
        "train_val_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.15,\n",
        "    stratify=df['family'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    train_val_df,\n",
        "    test_size=0.15/0.85,  # Val = 15% of total\n",
        "    stratify=train_val_df['family'],\n",
        "    random_state=42\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "uJAwfXb-CdNW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2005a541-3a77-4182-fb36-d73fc9a4baf4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Load CSV\\ndf = pd.read_csv(metadata_path)\\n\\n# Split into train/val/test (stratified by family)\\ntrain_val_df, test_df = train_test_split(\\n    df,\\n    test_size=0.15,\\n    stratify=df['family'],\\n    random_state=42\\n)\\n\\ntrain_df, val_df = train_test_split(\\n    train_val_df,\\n    test_size=0.15/0.85,  # Val = 15% of total\\n    stratify=train_val_df['family'],\\n    random_state=42\\n)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def copy_images(split_df, split_name):\n",
        "    \"\"\"Copy images to train/val/test folders using metadata.\"\"\"\n",
        "    for _, row in tqdm(split_df.iterrows(), desc=f\"Copying {split_name} images\"):\n",
        "        # Source path: Parse from 'file_path' column\n",
        "        src_path = os.path.join(dataset_dir, row['file_path'])\n",
        "\n",
        "        # Destination path: Use 'family' as class folder\n",
        "        dest_dir = os.path.join(output_dir, split_name, row['family'])\n",
        "        os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "        # Extract filename from 'file_path'\n",
        "        filename = os.path.basename(row['file_path'])\n",
        "        dest_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "        # Copy file\n",
        "        if os.path.exists(src_path):\n",
        "            copyfile(src_path, dest_path)\n",
        "        else:\n",
        "            print(f\"Warning: {src_path} not found\")\n",
        "\n",
        "# Copy images\n",
        "copy_images(train_df, \"train\")\n",
        "copy_images(val_df, \"val\")\n",
        "copy_images(test_df, \"test\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Jr7HxX1lCqSP",
        "outputId": "85bf179b-a022-4a2b-98f2-ef8e604794df"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef copy_images(split_df, split_name):\\n    \"\"\"Copy images to train/val/test folders using metadata.\"\"\"\\n    for _, row in tqdm(split_df.iterrows(), desc=f\"Copying {split_name} images\"):\\n        # Source path: Parse from \\'file_path\\' column\\n        src_path = os.path.join(dataset_dir, row[\\'file_path\\'])\\n\\n        # Destination path: Use \\'family\\' as class folder\\n        dest_dir = os.path.join(output_dir, split_name, row[\\'family\\'])\\n        os.makedirs(dest_dir, exist_ok=True)\\n\\n        # Extract filename from \\'file_path\\'\\n        filename = os.path.basename(row[\\'file_path\\'])\\n        dest_path = os.path.join(dest_dir, filename)\\n\\n        # Copy file\\n        if os.path.exists(src_path):\\n            copyfile(src_path, dest_path)\\n        else:\\n            print(f\"Warning: {src_path} not found\")\\n\\n# Copy images\\ncopy_images(train_df, \"train\")\\ncopy_images(val_df, \"val\")\\ncopy_images(test_df, \"test\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copying train images: 464it [07:56,  1.19it/s]Warning: /content/drive/MyDrive/Colab Notebooks/Deep Learning/dataset_rares_species/arthropoda_apidae/28260824_1065329_eol-full-size-copy.jpg not found\n",
        "Copying train images: 5104it [1:12:41,  1.17it/s]Warning: /content/drive/MyDrive/Colab Notebooks/Deep Learning/dataset_rares_species/arthropoda_apidae/28214440_1065290_eol-full-size-copy.jpg not found\n",
        "Copying train images: 7311it [1:43:10,  1.19it/s]Warning: /content/drive/MyDrive/Colab Notebooks/Deep Learning/dataset_rares_species/arthropoda_apidae/28260831_1065329_eol-full-size-copy.jpg not found\n",
        "Copying train images: 7968it [1:52:16,  1.18it/s]Warning: /content/drive/MyDrive/Colab Notebooks/Deep Learning/dataset_rares_species/arthropoda_apidae/28214384_1065290_eol-full-size-copy.jpg not found\n",
        "Copying train images: 8387it [1:58:04,  1.18it/s]\n",
        "Copying val images: 626it [08:41,  1.31it/s]Warning: /content/drive/MyDrive/Colab Notebooks/Deep Learning/dataset_rares_species/arthropoda_apidae/22375122_1065346_eol-full-size-copy.jpg not found\n",
        "Copying val images: 1798it [24:38,  1.22it/s]\n",
        "Copying test images: 98it [01:21,  1.15it/s]Warning: /content/drive/MyDrive/Colab Notebooks/Deep Learning/dataset_rares_species/arthropoda_apidae/28408134_1065346_eol-full-size-copy.jpg not found\n",
        "Copying test images: 1798it [24:39,  1.22it/s]"
      ],
      "metadata": {
        "id": "2VaSoq-P_lZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0 - Creating a subset of the data do the tests (we run this once)"
      ],
      "metadata": {
        "id": "-KcfDiuQPBcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define paths\n",
        "drive_root = '/content/drive/MyDrive/Colab Notebooks/Deep Learning'\n",
        "data_dir = os.path.join(drive_root, \"data\")  # Where your splits are stored\n",
        "exp_dir = os.path.join(drive_root, \"experimentation\")  # Where to save smaller dataset\n",
        "\n",
        "# Define target sizes\n",
        "train_size = 1676\n",
        "val_size = 358\n",
        "test_size = 358\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Create experimentation directories\n",
        "os.makedirs(os.path.join(exp_dir, \"train\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(exp_dir, \"val\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(exp_dir, \"test\"), exist_ok=True)\n",
        "\n",
        "def create_subset(src_dir, dest_dir, target_size):\n",
        "    \"\"\"Create a stratified subset of images maintaining class proportions.\"\"\"\n",
        "\n",
        "    # Get all class folders\n",
        "    class_folders = [f for f in os.listdir(src_dir) if os.path.isdir(os.path.join(src_dir, f))]\n",
        "\n",
        "    # Count total images\n",
        "    total_images = 0\n",
        "    class_counts = {}\n",
        "    for cls in class_folders:\n",
        "        cls_path = os.path.join(src_dir, cls)\n",
        "        image_files = [f for f in os.listdir(cls_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        class_counts[cls] = len(image_files)\n",
        "        total_images += len(image_files)\n",
        "\n",
        "    # Calculate how many images to take from each class\n",
        "    class_samples = {}\n",
        "    remaining = target_size\n",
        "    for cls, count in class_counts.items():\n",
        "        # Calculate proportional number (with at least 1 image per class)\n",
        "        samples = max(1, int(count * target_size / total_images))\n",
        "        class_samples[cls] = min(samples, count)  # Can't sample more than available\n",
        "        remaining -= class_samples[cls]\n",
        "\n",
        "    # Adjust to match exact target size\n",
        "    if remaining > 0:\n",
        "        # Distribute remaining samples to classes with more images\n",
        "        classes_sorted = sorted(class_counts.keys(), key=lambda c: class_counts[c], reverse=True)\n",
        "        for cls in classes_sorted:\n",
        "            if class_samples[cls] < class_counts[cls]:\n",
        "                class_samples[cls] += 1\n",
        "                remaining -= 1\n",
        "            if remaining == 0:\n",
        "                break\n",
        "    elif remaining < 0:\n",
        "        # Remove samples to match target size\n",
        "        classes_sorted = sorted(class_counts.keys(), key=lambda c: class_counts[c], reverse=True)\n",
        "        for cls in classes_sorted:\n",
        "            if class_samples[cls] > 1:  # Keep at least 1 sample\n",
        "                class_samples[cls] -= 1\n",
        "                remaining += 1\n",
        "            if remaining == 0:\n",
        "                break\n",
        "\n",
        "    # Copy images\n",
        "    for cls in tqdm(class_folders, desc=f\"Creating subset in {os.path.basename(dest_dir)}\"):\n",
        "        src_class_dir = os.path.join(src_dir, cls)\n",
        "        dest_class_dir = os.path.join(dest_dir, cls)\n",
        "        os.makedirs(dest_class_dir, exist_ok=True)\n",
        "\n",
        "        # Get all image files in this class\n",
        "        image_files = [f for f in os.listdir(src_class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        # Randomly sample the required number\n",
        "        samples_to_take = min(class_samples[cls], len(image_files))\n",
        "        sampled_files = random.sample(image_files, samples_to_take)\n",
        "\n",
        "        # Copy sampled files\n",
        "        for file in sampled_files:\n",
        "            src_file = os.path.join(src_class_dir, file)\n",
        "            dest_file = os.path.join(dest_class_dir, file)\n",
        "            shutil.copy2(src_file, dest_file)\n",
        "\n",
        "    # Count total copied\n",
        "    copied_total = 0\n",
        "    for cls in class_folders:\n",
        "        dest_class_dir = os.path.join(dest_dir, cls)\n",
        "        if os.path.exists(dest_class_dir):\n",
        "            copied_total += len(os.listdir(dest_class_dir))\n",
        "\n",
        "    print(f\"Copied {copied_total} images to {os.path.basename(dest_dir)}\")\n",
        "    return copied_total\n",
        "\n",
        "# Create subsets for each split\n",
        "print(\"Creating experimentation dataset...\")\n",
        "train_copied = create_subset(\n",
        "    os.path.join(data_dir, \"train\"),\n",
        "    os.path.join(exp_dir, \"train\"),\n",
        "    train_size\n",
        ")\n",
        "\n",
        "val_copied = create_subset(\n",
        "    os.path.join(data_dir, \"val\"),\n",
        "    os.path.join(exp_dir, \"val\"),\n",
        "    val_size\n",
        ")\n",
        "\n",
        "test_copied = create_subset(\n",
        "    os.path.join(data_dir, \"test\"),\n",
        "    os.path.join(exp_dir, \"test\"),\n",
        "    test_size\n",
        ")\n",
        "\n",
        "print(f\"\\nExperimentation dataset created:\")\n",
        "print(f\"Train: {train_copied} images\")\n",
        "print(f\"Validation: {val_copied} images\")\n",
        "print(f\"Test: {test_copied} images\")\n",
        "print(f\"Total: {train_copied + val_copied + test_copied} images\")\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "l50429WjO-oj",
        "outputId": "67bde788-a051-4598-cbb4-16a4d482f673"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport os\\nimport random\\nimport shutil\\nfrom pathlib import Path\\nfrom tqdm import tqdm\\n\\n# Define paths\\ndrive_root = \\'/content/drive/MyDrive/Colab Notebooks/Deep Learning\\'\\ndata_dir = os.path.join(drive_root, \"data\")  # Where your splits are stored\\nexp_dir = os.path.join(drive_root, \"experimentation\")  # Where to save smaller dataset\\n\\n# Define target sizes\\ntrain_size = 1676\\nval_size = 358\\ntest_size = 358\\n\\n# Set random seed for reproducibility\\nrandom.seed(42)\\n\\n# Create experimentation directories\\nos.makedirs(os.path.join(exp_dir, \"train\"), exist_ok=True)\\nos.makedirs(os.path.join(exp_dir, \"val\"), exist_ok=True)\\nos.makedirs(os.path.join(exp_dir, \"test\"), exist_ok=True)\\n\\ndef create_subset(src_dir, dest_dir, target_size):\\n    \"\"\"Create a stratified subset of images maintaining class proportions.\"\"\"\\n\\n    # Get all class folders\\n    class_folders = [f for f in os.listdir(src_dir) if os.path.isdir(os.path.join(src_dir, f))]\\n\\n    # Count total images\\n    total_images = 0\\n    class_counts = {}\\n    for cls in class_folders:\\n        cls_path = os.path.join(src_dir, cls)\\n        image_files = [f for f in os.listdir(cls_path) if f.lower().endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\'))]\\n        class_counts[cls] = len(image_files)\\n        total_images += len(image_files)\\n\\n    # Calculate how many images to take from each class\\n    class_samples = {}\\n    remaining = target_size\\n    for cls, count in class_counts.items():\\n        # Calculate proportional number (with at least 1 image per class)\\n        samples = max(1, int(count * target_size / total_images))\\n        class_samples[cls] = min(samples, count)  # Can\\'t sample more than available\\n        remaining -= class_samples[cls]\\n\\n    # Adjust to match exact target size\\n    if remaining > 0:\\n        # Distribute remaining samples to classes with more images\\n        classes_sorted = sorted(class_counts.keys(), key=lambda c: class_counts[c], reverse=True)\\n        for cls in classes_sorted:\\n            if class_samples[cls] < class_counts[cls]:\\n                class_samples[cls] += 1\\n                remaining -= 1\\n            if remaining == 0:\\n                break\\n    elif remaining < 0:\\n        # Remove samples to match target size\\n        classes_sorted = sorted(class_counts.keys(), key=lambda c: class_counts[c], reverse=True)\\n        for cls in classes_sorted:\\n            if class_samples[cls] > 1:  # Keep at least 1 sample\\n                class_samples[cls] -= 1\\n                remaining += 1\\n            if remaining == 0:\\n                break\\n\\n    # Copy images\\n    for cls in tqdm(class_folders, desc=f\"Creating subset in {os.path.basename(dest_dir)}\"):\\n        src_class_dir = os.path.join(src_dir, cls)\\n        dest_class_dir = os.path.join(dest_dir, cls)\\n        os.makedirs(dest_class_dir, exist_ok=True)\\n\\n        # Get all image files in this class\\n        image_files = [f for f in os.listdir(src_class_dir) if f.lower().endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\'))]\\n\\n        # Randomly sample the required number\\n        samples_to_take = min(class_samples[cls], len(image_files))\\n        sampled_files = random.sample(image_files, samples_to_take)\\n\\n        # Copy sampled files\\n        for file in sampled_files:\\n            src_file = os.path.join(src_class_dir, file)\\n            dest_file = os.path.join(dest_class_dir, file)\\n            shutil.copy2(src_file, dest_file)\\n\\n    # Count total copied\\n    copied_total = 0\\n    for cls in class_folders:\\n        dest_class_dir = os.path.join(dest_dir, cls)\\n        if os.path.exists(dest_class_dir):\\n            copied_total += len(os.listdir(dest_class_dir))\\n\\n    print(f\"Copied {copied_total} images to {os.path.basename(dest_dir)}\")\\n    return copied_total\\n\\n# Create subsets for each split\\nprint(\"Creating experimentation dataset...\")\\ntrain_copied = create_subset(\\n    os.path.join(data_dir, \"train\"),\\n    os.path.join(exp_dir, \"train\"),\\n    train_size\\n)\\n\\nval_copied = create_subset(\\n    os.path.join(data_dir, \"val\"),\\n    os.path.join(exp_dir, \"val\"),\\n    val_size\\n)\\n\\ntest_copied = create_subset(\\n    os.path.join(data_dir, \"test\"),\\n    os.path.join(exp_dir, \"test\"),\\n    test_size\\n)\\n\\nprint(f\"\\nExperimentation dataset created:\")\\nprint(f\"Train: {train_copied} images\")\\nprint(f\"Validation: {val_copied} images\")\\nprint(f\"Test: {test_copied} images\")\\nprint(f\"Total: {train_copied + val_copied + test_copied} images\")\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating experimentation dataset...\n",
        "\n",
        "Creating subset in train: 100%|██████████| 202/202 [03:16<00:00,  1.03it/s]\n",
        "\n",
        "Copied 1676 images to train\n",
        "\n",
        "Creating subset in val: 100%|██████████| 202/202 [02:49<00:00,  1.19it/s]\n",
        "\n",
        "Copied 358 images to val\n",
        "\n",
        "Creating subset in test: 100%|██████████| 202/202 [02:48<00:00,  1.20it/s]\n",
        "\n",
        "Copied 358 images to test\n",
        "\n",
        "Experimentation dataset created:\n",
        "\n",
        "- Train: 1676 images\n",
        "\n",
        "- Validation: 358 images\n",
        "\n",
        "- Test: 358 images\n",
        "\n",
        "- Total: 2392 images\n"
      ],
      "metadata": {
        "id": "-FMCxBABSPP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup and Imports"
      ],
      "metadata": {
        "id": "i-CRuHDtwrI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Importing libraries"
      ],
      "metadata": {
        "id": "tEE_hCmAkAdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-cv\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6onOI9wCftt",
        "outputId": "a4611b57-c287-4942-9e53-1cb8e80e8485"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-cv in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-cv) (24.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras-cv) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from keras-cv) (2024.11.6)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (from keras-cv) (4.9.8)\n",
            "Requirement already satisfied: keras-core in /usr/local/lib/python3.11/dist-packages (from keras-cv) (0.1.7)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from keras-cv) (0.3.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-cv) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-cv) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-cv) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (3.13.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (0.1.9)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (0.7.1)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (1.12.2)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (5.29.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (18.1.0)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (1.16.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (3.0.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (1.17.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (4.13.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-cv) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-cv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-cv) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-cv) (2025.1.31)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->keras-core->keras-cv) (25.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from promise->tensorflow-datasets->keras-cv) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core->keras-cv) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core->keras-cv) (2.18.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow-datasets->keras-cv) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv) (1.69.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-cv) (0.1.2)\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.11/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.13.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras_cv\n",
        "import keras_tuner as kt\n",
        "from keras_tuner import HyperModel, Hyperband\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from keras import Model, Sequential, layers\n",
        "from keras.applications import ResNet50, MobileNetV2, EfficientNetB0, EfficientNetB7, DenseNet121, DenseNet169, DenseNet201\n",
        "from keras.layers import (Dense, GlobalAveragePooling2D, Dropout, Conv2D, MaxPooling2D,\n",
        "                          Flatten, add, RandomFlip, RandomRotation, RandomBrightness,\n",
        "                          RandomContrast, RandomTranslation, RandomZoom, LayerNormalization)\n",
        "\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from keras.metrics import CategoricalAccuracy, AUC, F1Score\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, EarlyStopping, Callback\n",
        "from keras.preprocessing import image_dataset_from_directory\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from shutil import copyfile\n",
        "from tqdm import tqdm\n",
        "import math"
      ],
      "metadata": {
        "id": "NOwNdSWMw7Hf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 - Dataset Loading and Preprocessing"
      ],
      "metadata": {
        "id": "YEiJl_sV2Wd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths to your organized data directories\n",
        "\n",
        "# Define base paths\n",
        "drive_root = '/content/drive/MyDrive/Colab Notebooks/Deep Learning'\n",
        "\n",
        "# Path to original metadata CSV (only used for EDA, not required for training)\n",
        "metadata_path = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/species_metadata.csv'\n",
        "\n",
        "# Configure for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Load metadata from CSV (optional, only for EDA visualizations)\n",
        "df = pd.read_csv(metadata_path)\n"
      ],
      "metadata": {
        "id": "JWjwI4sgatvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "321d0f4e-9c0c-4893-f796-70128e721d81"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Flag to control whether to use the full dataset or experimental subset\n",
        "USE_SUBSET = True  # Set to False to use full datasets\n",
        "\n",
        "# Choose the appropriate source directory based on the flag\n",
        "if USE_SUBSET:\n",
        "    data_source = os.path.join(drive_root, \"experimentation\")\n",
        "    print(\"Using EXPERIMENTAL SUBSET for faster training\")\n",
        "else:\n",
        "    data_source = os.path.join(drive_root, \"data\")\n",
        "    print(\"Using FULL DATASET for final training\")\n",
        "\n",
        "# Load datasets directly from the chosen directory\n",
        "train_ds = image_dataset_from_directory(\n",
        "    os.path.join(data_source, \"train\"),\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=64,\n",
        "    image_size=(224, 224),\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_ds = image_dataset_from_directory(\n",
        "    os.path.join(data_source, \"val\"),\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=64,\n",
        "    image_size=(224, 224),\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_ds = image_dataset_from_directory(\n",
        "    os.path.join(data_source, \"test\"),\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=64,\n",
        "    image_size=(224, 224),\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Store the class names BEFORE applying any transformations\n",
        "num_classes = len(train_ds.class_names)\n",
        "class_names = train_ds.class_names\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Class names: {class_names[:5]}... (showing first 5)\")\n",
        "\n",
        "# Configure datasets for performance\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Apply normalization preprocessing\n",
        "normalization_layer = layers.Rescaling(1./255)\n",
        "\n",
        "# Apply preprocessing to the datasets\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
        "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# Print dataset info\n",
        "print(\"Dataset information:\")\n",
        "print(f\"Train dataset: {train_ds}\")\n",
        "print(f\"Validation dataset: {val_ds}\")\n",
        "print(f\"Test dataset: {test_ds}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ### Explanation of Dataset Loading\n",
        "#\n",
        "# We load our datasets directly from the organized directory structure:\n",
        "#\n",
        "# 1. **Directory Structure**:\n",
        "#    - The `output_dir` points to a directory containing three subdirectories: \"train\", \"val\", and \"test\"\n",
        "#    - Within each subdirectory, there's a folder for each family (class)\n",
        "#    - Images for each family are stored in their respective family folders\n",
        "#\n",
        "# 2. **Metadata CSV** (Optional):\n",
        "#    - The metadata CSV is loaded only for exploratory data analysis (EDA)\n",
        "#    - It's not required for the actual model training since the class information comes from the directory structure\n",
        "#    - If the CSV is unavailable, the code will still work but some visualizations might be limited\n",
        "#\n",
        "# 3. **Dataset Loading**:\n",
        "#    - We use `image_dataset_from_directory` which automatically:\n",
        "#      - Detects classes from folder names\n",
        "#      - Loads images from each class folder\n",
        "#      - Assigns the correct labels based on the folder structure\n",
        "#    - Images are resized to 224×224 pixels to match the input size expected by the pre-trained models\n",
        "#\n",
        "# 4. **Dataset Optimization**:\n",
        "#    - We apply normalization to scale pixel values from [0-255] to [0-1]\n",
        "#    - We use prefetching to overlap data preprocessing and model execution for better performance"
      ],
      "metadata": {
        "id": "vzlflXIc2ExZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba7ba6a-36ed-450b-c14c-97fa6b95c5cc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using EXPERIMENTAL SUBSET for faster training\n",
            "Found 1676 files belonging to 202 classes.\n",
            "Found 358 files belonging to 202 classes.\n",
            "Found 358 files belonging to 202 classes.\n",
            "Number of classes: 202\n",
            "Class names: ['accipitridae', 'acipenseridae', 'acroporidae', 'agamidae', 'agariciidae']... (showing first 5)\n",
            "Dataset information:\n",
            "Train dataset: <_ParallelMapDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 202), dtype=tf.float32, name=None))>\n",
            "Validation dataset: <_ParallelMapDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 202), dtype=tf.float32, name=None))>\n",
            "Test dataset: <_ParallelMapDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 202), dtype=tf.float32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Check and verify GPU availability"
      ],
      "metadata": {
        "id": "6oIoIJ1wzInc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DomgFb0FzMoC",
        "outputId": "0a829545-14f4-40b4-e213-26c3a302c6e8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr  4 11:22:06 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0             28W /   70W |    8308MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k12tT_bGiOHj",
        "outputId": "04470260-2684-409b-8579-7bd8beb7a8cd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Very quick EDA"
      ],
      "metadata": {
        "id": "e_b2wYsDknjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualize family distribution\n",
        "family_counts = df['family'].value_counts()\n",
        "family_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "Yc2c0qTkkmSI",
        "outputId": "a7f948a5-6ee4-45be-ab15-9c36bc967cf2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "family\n",
              "dactyloidae        300\n",
              "cercopithecidae    300\n",
              "formicidae         291\n",
              "carcharhinidae     270\n",
              "salamandridae      270\n",
              "                  ... \n",
              "cyprinodontidae     30\n",
              "alligatoridae       30\n",
              "balaenidae          30\n",
              "goodeidae           30\n",
              "siluridae           29\n",
              "Name: count, Length: 202, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>family</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>dactyloidae</th>\n",
              "      <td>300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cercopithecidae</th>\n",
              "      <td>300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>formicidae</th>\n",
              "      <td>291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carcharhinidae</th>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>salamandridae</th>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cyprinodontidae</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>alligatoridae</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>balaenidae</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>goodeidae</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>siluridae</th>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>202 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Helper Classes and Functions"
      ],
      "metadata": {
        "id": "rv1UC5-Mz2Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Pipeline class (similar to Sequential but more explicitly for preprocessing)\n",
        "#we use this because of pipeline augmentation class\n",
        "class Pipeline(tf.keras.Model):\n",
        "    def __init__(self, layers, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.layers_list = layers\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = inputs\n",
        "        for layer in self.layers_list:\n",
        "            x = layer(x, training=training)\n",
        "        return x\n",
        "\n",
        "def exp_decay_lr_scheduler(epoch, lr):\n",
        "    \"\"\"Exponential learning rate decay\"\"\"\n",
        "    return float(lr * math.exp(-0.1 * epoch))\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ### Explanation of Helper Classes and Functions\n",
        "#\n",
        "# 1. **Pipeline Class**:\n",
        "#    - This is a custom implementation of a sequential pipeline for data augmentation\n",
        "#    - Unlike the standard Keras Sequential, it explicitly passes the training flag to each layer\n",
        "#    - This is important for layers that behave differently during training and inference\n",
        "#\n",
        "# 2. **exp_decay_lr_scheduler Function**:\n",
        "#    - Implements exponential learning rate decay\n",
        "#    - Reduces the learning rate by a factor of e^(-0.1) each epoch\n",
        "#    - This helps the model converge more precisely as training progresses\n"
      ],
      "metadata": {
        "id": "udIJGt4D0DKy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Defining Data Augmentation Strategies"
      ],
      "metadata": {
        "id": "yGoxkj7g6lLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define different augmentation pipelines with varying levels of complexity.\n",
        "\n",
        "# Define value range for augmentation\n",
        "value_range = (0.0, 1.0)\n",
        "\n",
        "# Define different augmentation pipelines\n",
        "\n",
        "# 1. Simple Augmentation Pipeline\n",
        "simple_augmentation = Pipeline(\n",
        "    [\n",
        "        RandomBrightness(factor=0.1, value_range=value_range),\n",
        "        RandomFlip(\"horizontal\"),\n",
        "        RandomRotation(factor=0.1, fill_mode=\"reflect\")\n",
        "    ],\n",
        "    name=\"simple_augmentation\"\n",
        ")\n",
        "\n",
        "# 2. Medium Augmentation Pipeline\n",
        "medium_augmentation = Pipeline(\n",
        "    [\n",
        "        RandomBrightness(factor=0.15, value_range=value_range),\n",
        "        RandomFlip(\"horizontal\"),\n",
        "        RandomRotation(factor=0.15, fill_mode=\"reflect\"),\n",
        "        RandomContrast(factor=0.1, value_range=value_range),\n",
        "        RandomTranslation(height_factor=0.1, width_factor=0.1, fill_mode=\"reflect\")\n",
        "    ],\n",
        "    name=\"medium_augmentation\"\n",
        ")\n",
        "\n",
        "# 3. Complex Augmentation Pipeline\n",
        "complex_augmentation = Pipeline(\n",
        "    [\n",
        "        RandomBrightness(factor=0.2, value_range=value_range),\n",
        "        RandomFlip(\"horizontal\"),\n",
        "        RandomFlip(\"vertical\"),\n",
        "        RandomRotation(factor=0.3, fill_mode=\"reflect\"),\n",
        "        RandomContrast(factor=0.2, value_range=value_range),\n",
        "        RandomTranslation(height_factor=0.15, width_factor=0.15, fill_mode=\"reflect\"),\n",
        "        RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2), fill_mode=\"reflect\"),\n",
        "\n",
        "    ],\n",
        "    name=\"complex_augmentation\"\n",
        ")\n",
        "# Function to select augmentation based on complexity level\n",
        "def get_augmentation(complexity=\"medium\"):\n",
        "    \"\"\"\n",
        "    Get augmentation pipeline based on desired complexity\n",
        "\n",
        "    Args:\n",
        "        complexity: One of \"simple\", \"medium\", or \"complex\"\n",
        "\n",
        "    Returns:\n",
        "        Selected augmentation pipeline\n",
        "    \"\"\"\n",
        "    if complexity.lower() == \"simple\":\n",
        "        return simple_augmentation\n",
        "    elif complexity.lower() == \"complex\":\n",
        "        return complex_augmentation\n",
        "    else:\n",
        "        # Default to medium\n",
        "        return medium_augmentation\n",
        "\n",
        "# %% [markdown]\n",
        "# ### Explanation of Data Augmentation Strategies\n",
        "#\n",
        "# Data augmentation is crucial for improving model generalization, especially with limited data. We define three levels of augmentation complexity:\n",
        "#\n",
        "# 1. **Simple Augmentation**:\n",
        "#    - Basic transformations: horizontal flips, small rotations, and brightness adjustments\n",
        "#    - Conservative approach that preserves most image characteristics\n",
        "#    - Best for: Models that are sensitive to distortion (like Vision Transformers)\n",
        "#\n",
        "# 2. **Medium Augmentation**:\n",
        "#    - Adds contrast variation and image translation\n",
        "#    - Moderate level of image distortion\n",
        "#    - Best for: General purpose use with standard CNN architectures\n",
        "#\n",
        "# 3. **Complex Augmentation**:\n",
        "#    - Adds vertical flips, larger rotations, zooming, and random erasing\n",
        "#    - Aggressive augmentation for maximum diversity\n",
        "#    - Best for: Smaller models that benefit from seeing more varied examples\n",
        "#\n",
        "# The visualizations show how these pipelines transform images:\n",
        "# - The first visualization compares how each augmentation pipeline affects the same image\n",
        "# - The second visualization shows multiple different variations created by the medium augmentation pipeline\n",
        "#\n",
        "# These transformations help our models learn robust features that generalize beyond the specific training examples, effectively increasing our training data diversity.\n",
        "\n"
      ],
      "metadata": {
        "id": "MJjTx6Lc6OA1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Architecture"
      ],
      "metadata": {
        "id": "n87BDsCnDh8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet"
      ],
      "metadata": {
        "id": "A44Y4zFku2HD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RareSpeciesResNet(Model):\n",
        "    \"\"\"\n",
        "    Pre-trained ResNet-50 with data augmentation for rare species classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, unfrozen_layers=10, augmentation_type=\"medium\"):\n",
        "        \"\"\"\n",
        "        Initialization\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Get the appropriate augmentation pipeline\n",
        "        self.augmentation_layer = get_augmentation(augmentation_type)\n",
        "\n",
        "        # Pre-trained architecture\n",
        "        self.pre_trained_architecture = ResNet50(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(224, 224, 3)\n",
        "        )\n",
        "\n",
        "        # Freeze all layers except the last unfrozen_layers\n",
        "        for layer in self.pre_trained_architecture.layers[:-unfrozen_layers]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # Final layers for classification\n",
        "        self.global_pool_layer = GlobalAveragePooling2D()\n",
        "        self.dense_layer = Dense(self.num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        Forward call\n",
        "        \"\"\"\n",
        "        # Apply augmentation only during training\n",
        "        if training:\n",
        "            x = self.augmentation_layer(inputs, training=training)\n",
        "        else:\n",
        "            x = inputs\n",
        "\n",
        "        # Pre-trained model\n",
        "        x = self.pre_trained_architecture(x)\n",
        "        x = self.global_pool_layer(x)\n",
        "\n",
        "        return self.dense_layer(x)\n"
      ],
      "metadata": {
        "id": "0U1A7SVtvRyk"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MobileNet"
      ],
      "metadata": {
        "id": "FhYs3iJhvAm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RareSpeciesMobileNet(Model):\n",
        "    \"\"\"\n",
        "    Pre-trained MobileNetV2 with data augmentation for rare species classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, unfrozen_layers=15, augmentation_type=\"medium\"):\n",
        "        \"\"\"\n",
        "        Initialization\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Get the appropriate augmentation pipeline\n",
        "        self.augmentation_layer = get_augmentation(augmentation_type)\n",
        "\n",
        "        # Pre-trained architecture\n",
        "        self.pre_trained_architecture = MobileNetV2(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(224, 224, 3)\n",
        "        )\n",
        "\n",
        "        # Freeze all layers except the last unfrozen_layers\n",
        "        for layer in self.pre_trained_architecture.layers[:-unfrozen_layers]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # Final layers for classification\n",
        "        self.global_pool_layer = GlobalAveragePooling2D()\n",
        "        self.dropout_layer = Dropout(0.2)\n",
        "        self.dense_layer = Dense(self.num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        Forward call\n",
        "        \"\"\"\n",
        "        # Apply augmentation only during training\n",
        "        if training:\n",
        "            x = self.augmentation_layer(inputs, training=training)\n",
        "        else:\n",
        "            x = inputs\n",
        "\n",
        "        # Pre-trained model\n",
        "        x = self.pre_trained_architecture(x)\n",
        "        x = self.global_pool_layer(x)\n",
        "        x = self.dropout_layer(x, training=training)\n",
        "\n",
        "        return self.dense_layer(x)"
      ],
      "metadata": {
        "id": "tD48T_SuvK-Y"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vit Model"
      ],
      "metadata": {
        "id": "zd_2EiSSvhkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RareSpeciesViT(Model):\n",
        "    \"\"\"\n",
        "    Pre-trained Vision Transformer with data augmentation for rare species classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, unfrozen_layers=2, augmentation_type=\"simple\"):\n",
        "        \"\"\"\n",
        "        Initialization\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Get the appropriate augmentation pipeline\n",
        "        self.augmentation_layer = get_augmentation(augmentation_type)\n",
        "\n",
        "        # Load ViT from TensorFlow Hub\n",
        "        self.pre_trained_architecture = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/sayakpaul/vit_b16_fe/1\",\n",
        "            trainable=(unfrozen_layers > 0)\n",
        "        )\n",
        "\n",
        "        # Final layer for classification\n",
        "        self.dense_layer = Dense(self.num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        Forward call\n",
        "        \"\"\"\n",
        "        # Apply augmentation only during training\n",
        "        if training:\n",
        "            x = self.augmentation_layer(inputs, training=training)\n",
        "        else:\n",
        "            x = inputs\n",
        "\n",
        "        # Pre-trained model\n",
        "        x = self.pre_trained_architecture(x)\n",
        "\n",
        "        return self.dense_layer(x)"
      ],
      "metadata": {
        "id": "omZjUjtTvlE4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EfficientNet"
      ],
      "metadata": {
        "id": "L1sXMrM84GcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RareSpeciesEfficientNet(Model):\n",
        "    \"\"\"\n",
        "    Pre-trained EfficientNet with data augmentation for rare species classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, unfrozen_layers=10, augmentation_type=\"medium\",\n",
        "                 variant=\"B7\"):\n",
        "        \"\"\"\n",
        "        Initialization\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Get the appropriate augmentation pipeline\n",
        "        self.augmentation_layer = get_augmentation(augmentation_type)\n",
        "\n",
        "\n",
        "        base_model_fn = EfficientNetB7\n",
        "\n",
        "\n",
        "        # Pre-trained architecture\n",
        "        self.pre_trained_architecture = base_model_fn(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(224, 224, 3)\n",
        "        )\n",
        "\n",
        "        # Freeze all layers except the last unfrozen_layers\n",
        "        if unfrozen_layers > 0:\n",
        "            for layer in self.pre_trained_architecture.layers[:-unfrozen_layers]:\n",
        "                layer.trainable = False\n",
        "        else:\n",
        "            # Freeze all layers\n",
        "            for layer in self.pre_trained_architecture.layers:\n",
        "                layer.trainable = False\n",
        "\n",
        "        # Final layers for classification\n",
        "        self.global_pool_layer = GlobalAveragePooling2D()\n",
        "        self.dropout_layer = Dropout(0.3)  # Higher dropout to combat overfitting\n",
        "        self.dense_layer = Dense(self.num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        Forward call\n",
        "        \"\"\"\n",
        "        # Apply augmentation only during training\n",
        "        if training:\n",
        "            x = self.augmentation_layer(inputs, training=training)\n",
        "        else:\n",
        "            x = inputs\n",
        "\n",
        "        # Pre-trained model\n",
        "        x = self.pre_trained_architecture(x)\n",
        "        x = self.global_pool_layer(x)\n",
        "        x = self.dropout_layer(x, training=training)\n",
        "\n",
        "        return self.dense_layer(x)"
      ],
      "metadata": {
        "id": "egCWPvkK4A8h"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Densenet"
      ],
      "metadata": {
        "id": "ZCzq2M9qOi9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RareSpeciesDenseNet(Model):\n",
        "    \"\"\"\n",
        "    Pre-trained DenseNet with data augmentation for rare species classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, unfrozen_layers=10, augmentation_type=\"medium\",\n",
        "                 variant=\"201\"):\n",
        "        \"\"\"\n",
        "        Initialization\n",
        "\n",
        "        Args:\n",
        "            num_classes: Number of species classes to classify\n",
        "            unfrozen_layers: Number of layers to unfreeze for fine-tuning\n",
        "            augmentation_type: Type of augmentation pipeline to use\n",
        "            variant: DenseNet variant (121, 169, or 201)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Get the appropriate augmentation pipeline\n",
        "        self.augmentation_layer = get_augmentation(augmentation_type)\n",
        "\n",
        "\n",
        "        base_model_fn = DenseNet201\n",
        "\n",
        "\n",
        "        # Pre-trained architecture\n",
        "        self.pre_trained_architecture = base_model_fn(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(224, 224, 3)\n",
        "        )\n",
        "\n",
        "        # Freeze all layers except the last unfrozen_layers\n",
        "        if unfrozen_layers > 0:\n",
        "            total_layers = len(self.pre_trained_architecture.layers)\n",
        "            for layer in self.pre_trained_architecture.layers[:-unfrozen_layers]:\n",
        "                layer.trainable = False\n",
        "            for layer in self.pre_trained_architecture.layers[-unfrozen_layers:]:\n",
        "                layer.trainable = True\n",
        "            print(f\"Unfrozen {unfrozen_layers} of {total_layers} layers in DenseNet{variant}\")\n",
        "        else:\n",
        "            # Freeze all layers\n",
        "            for layer in self.pre_trained_architecture.layers:\n",
        "                layer.trainable = False\n",
        "            print(f\"All layers in DenseNet{variant} are frozen\")\n",
        "\n",
        "        # Final layers for classification\n",
        "        self.global_pool_layer = GlobalAveragePooling2D()\n",
        "        self.dropout_layer = Dropout(0.3)  # Higher dropout for regularization\n",
        "        self.dense_layer = Dense(self.num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        Forward call\n",
        "\n",
        "        Args:\n",
        "            inputs: Input tensor of shape [batch_size, height, width, channels]\n",
        "            training: Boolean indicating whether in training mode\n",
        "\n",
        "        Returns:\n",
        "            Class probability predictions\n",
        "        \"\"\"\n",
        "        # Apply augmentation only during training\n",
        "        if training:\n",
        "            x = self.augmentation_layer(inputs, training=training)\n",
        "        else:\n",
        "            x = inputs\n",
        "\n",
        "        # Pre-trained model\n",
        "        x = self.pre_trained_architecture(x)\n",
        "        x = self.global_pool_layer(x)\n",
        "        x = self.dropout_layer(x, training=training)\n",
        "\n",
        "        return self.dense_layer(x)"
      ],
      "metadata": {
        "id": "0lEJBmjbOq85"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Model Training Function"
      ],
      "metadata": {
        "id": "G-aqkam8EYmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RareSpeciesHyperModel(HyperModel):\n",
        "    \"\"\"\n",
        "    HyperModel for rare species classification that works with existing model classes\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, model_type='resnet'):\n",
        "        self.num_classes = num_classes\n",
        "        self.model_type = model_type\n",
        "\n",
        "    def build(self, hp):\n",
        "        \"\"\"\n",
        "        Build model with hyperparameters\n",
        "        \"\"\"\n",
        "        # Tune the augmentation type\n",
        "        augmentation_type = hp.Choice(\n",
        "            'augmentation_type',\n",
        "            values=['simple', 'medium', 'complex'],\n",
        "            default='medium'\n",
        "        )\n",
        "\n",
        "        # Tune the number of unfrozen layers\n",
        "        unfrozen_layers = hp.Int(\n",
        "            'unfrozen_layers',\n",
        "            min_value=0,\n",
        "            max_value=20,\n",
        "            default=5,\n",
        "            step=1\n",
        "        )\n",
        "\n",
        "        # Create model based on type\n",
        "        if self.model_type == 'resnet':\n",
        "            model = RareSpeciesResNet(\n",
        "                num_classes=self.num_classes,\n",
        "                unfrozen_layers=unfrozen_layers,\n",
        "                augmentation_type=augmentation_type\n",
        "            )\n",
        "        elif self.model_type == 'mobilenet':\n",
        "            model = RareSpeciesMobileNet(\n",
        "                num_classes=self.num_classes,\n",
        "                unfrozen_layers=unfrozen_layers,\n",
        "                augmentation_type=augmentation_type\n",
        "            )\n",
        "        elif self.model_type == 'vit':\n",
        "            model = RareSpeciesViT(\n",
        "                num_classes=self.num_classes,\n",
        "                unfrozen_layers=unfrozen_layers > 0,  # ViT uses boolean for trainable\n",
        "                augmentation_type=augmentation_type\n",
        "            )\n",
        "        elif self.model_type == 'efficientnet':\n",
        "            # Additional parameter for EfficientNet variant\n",
        "            variant = hp.Choice(\n",
        "                'efficientnet_variant',\n",
        "                values=['B0', 'B1', 'B2', 'B3'],\n",
        "                default='B0'\n",
        "            )\n",
        "\n",
        "            model = RareSpeciesEfficientNet(\n",
        "                num_classes=self.num_classes,\n",
        "                unfrozen_layers=unfrozen_layers,\n",
        "                augmentation_type=augmentation_type,\n",
        "                variant=variant\n",
        "            )\n",
        "        elif self.model_type == 'densenet':\n",
        "            # Additional parameter for DenseNet variant\n",
        "            variant = hp.Choice(\n",
        "                'densenet_variant',\n",
        "                values=['121', '169', '201'],\n",
        "                default='121'\n",
        "            )\n",
        "\n",
        "            model = RareSpeciesDenseNet(\n",
        "                num_classes=self.num_classes,\n",
        "                unfrozen_layers=unfrozen_layers,\n",
        "                augmentation_type=augmentation_type,\n",
        "                variant=variant\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "        # Tune batch size\n",
        "        batch_size = hp.Choice(\n",
        "            'batch_size',\n",
        "            values=[8, 16, 32, 64],\n",
        "            default=32\n",
        "        )\n",
        "\n",
        "        # Tune learning rate\n",
        "        learning_rate = hp.Float(\n",
        "            'learning_rate',\n",
        "            min_value=1e-5,\n",
        "            max_value=1e-2,\n",
        "            sampling='log',\n",
        "            default=1e-3\n",
        "        )\n",
        "\n",
        "        # Tune optimizer\n",
        "        optimizer_type = hp.Choice(\n",
        "            'optimizer',\n",
        "            values=['sgd', 'adam'],\n",
        "            default='sgd'\n",
        "        )\n",
        "\n",
        "        if optimizer_type == 'sgd':\n",
        "            # Tune momentum for SGD\n",
        "            momentum = hp.Float(\n",
        "                'momentum',\n",
        "                min_value=0.7,\n",
        "                max_value=0.99,\n",
        "                default=0.9,\n",
        "                step=0.05\n",
        "            )\n",
        "\n",
        "            # Tune weight decay for SGD\n",
        "            weight_decay = hp.Float(\n",
        "                'weight_decay',\n",
        "                min_value=1e-6,\n",
        "                max_value=1e-3,\n",
        "                sampling='log',\n",
        "                default=1e-4\n",
        "            )\n",
        "\n",
        "            optimizer = SGD(\n",
        "                learning_rate=learning_rate,\n",
        "                momentum=momentum,\n",
        "                weight_decay=weight_decay,\n",
        "                nesterov=True\n",
        "            )\n",
        "        else:\n",
        "            optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=[\n",
        "                'accuracy',\n",
        "                tf.keras.metrics.AUC(),\n",
        "                tf.keras.metrics.F1Score(average='macro', name='f1_score')\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Store batch size as a model attribute\n",
        "        model.batch_size = batch_size\n",
        "\n",
        "        return model\n",
        "\n",
        "def run_hyperband_tuning(train_ds, val_ds, test_ds, model_type='densenet', max_epochs=30,\n",
        "                         num_classes=400, project_name='rare_species'):\n",
        "    \"\"\"\n",
        "    Run Hyperband hyperparameter tuning for a specific model type\n",
        "    \"\"\"\n",
        "    # Create directory for results\n",
        "    results_dir = f'hyperband_results/{model_type}'\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize the HyperModel\n",
        "    hypermodel = RareSpeciesHyperModel(\n",
        "        num_classes=num_classes,\n",
        "        model_type=model_type\n",
        "    )\n",
        "\n",
        "    # Configure the Hyperband tuner\n",
        "    tuner = Hyperband(\n",
        "        hypermodel,\n",
        "        objective='val_accuracy',\n",
        "        max_epochs=max_epochs,\n",
        "        factor=3,\n",
        "        directory=results_dir,\n",
        "        project_name=f'{project_name}_{model_type}'\n",
        "    )\n",
        "\n",
        "    # Print search space summary\n",
        "    tuner.search_space_summary()\n",
        "\n",
        "    # Define early stopping callback\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # CSV Logger to track training progress\n",
        "    csv_logger = tf.keras.callbacks.CSVLogger(\n",
        "        f'{results_dir}/{model_type}_tuning.csv',\n",
        "        append=True\n",
        "    )\n",
        "\n",
        "    # Run the search\n",
        "    print(f\"\\nStarting Hyperband tuning for {model_type} model...\")\n",
        "    print(f\"Results will be saved to {results_dir}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Standard tuner search with only standard callbacks\n",
        "    tuner.search(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        callbacks=[early_stopping, csv_logger]\n",
        "    )\n",
        "\n",
        "    tuning_time = time.time() - start_time\n",
        "    print(f\"Hyperband tuning completed in {tuning_time:.2f} seconds\")\n",
        "\n",
        "    # Get best hyperparameters\n",
        "    best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "    print(f\"\\nBest hyperparameters:\")\n",
        "    for param, value in best_hp.values.items():\n",
        "        print(f\"- {param}: {value}\")\n",
        "\n",
        "    # Build model with best hyperparameters\n",
        "    best_model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "    # Save best hyperparameters to JSON\n",
        "    import json\n",
        "    with open(f'{results_dir}/best_hyperparameters.json', 'w') as f:\n",
        "        json.dump(best_hp.values, f, indent=2)\n",
        "\n",
        "    # Train the model for a few epochs to evaluate metrics\n",
        "    print(\"\\nTraining model with best hyperparameters to collect metrics...\")\n",
        "\n",
        "    # Getting the batch size\n",
        "    batch_size = best_hp.values.get('batch_size', 32)\n",
        "\n",
        "    # Simple metrics collection - we'll use a regular callback\n",
        "    class SimpleMetricsCollector(tf.keras.callbacks.Callback):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.metrics = {}\n",
        "            self.start_time = time.time()\n",
        "\n",
        "        def on_train_end(self, logs=None):\n",
        "            self.metrics['training_time'] = time.time() - self.start_time\n",
        "\n",
        "    metrics_collector = SimpleMetricsCollector()\n",
        "\n",
        "    # Train for a few epochs to get stable metrics\n",
        "    history = best_model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=5,  # Just a few epochs to get stable metrics\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[metrics_collector]\n",
        "    )\n",
        "\n",
        "    # Evaluate on train and validation sets\n",
        "    train_metrics = best_model.evaluate(train_ds, return_dict=True, verbose=0)\n",
        "    val_metrics = best_model.evaluate(val_ds, return_dict=True, verbose=0)\n",
        "\n",
        "    # Calculate overfitting score\n",
        "    overfitting = train_metrics['accuracy'] - val_metrics['accuracy']\n",
        "\n",
        "    # Print metrics table\n",
        "    print(\"\\nMetrics:\")\n",
        "    print(\"CV F1 Macro Val | CV F1 Macro Train | Accuracy Val | Accuracy Train | Loss Val | Loss Train | Overfitting | Time (s)\")\n",
        "    print(f\"{val_metrics['f1_score']:.4f} | {train_metrics['f1_score']:.4f} | \" +\n",
        "          f\"{val_metrics['accuracy']:.4f} | {train_metrics['accuracy']:.4f} | \" +\n",
        "          f\"{val_metrics['loss']:.4f} | {train_metrics['loss']:.4f} | \" +\n",
        "          f\"{overfitting:.4f} | {metrics_collector.metrics['training_time']:.2f}\")\n",
        "\n",
        "    # Create a DataFrame with metrics\n",
        "    metrics_dict = {\n",
        "        'val_f1_score': val_metrics['f1_score'],\n",
        "        'train_f1_score': train_metrics['f1_score'],\n",
        "        'val_accuracy': val_metrics['accuracy'],\n",
        "        'train_accuracy': train_metrics['accuracy'],\n",
        "        'val_loss': val_metrics['loss'],\n",
        "        'train_loss': train_metrics['loss'],\n",
        "        'overfitting': overfitting,\n",
        "        'training_time': metrics_collector.metrics['training_time']\n",
        "    }\n",
        "\n",
        "    # Add hyperparameters to the metrics\n",
        "    for param, value in best_hp.values.items():\n",
        "        metrics_dict[param] = value\n",
        "\n",
        "    metrics_df = pd.DataFrame([metrics_dict])\n",
        "\n",
        "    # If test_ds is provided, evaluate on it\n",
        "    if test_ds is not None:\n",
        "        print(\"\\nEvaluating on test set...\")\n",
        "        test_metrics = best_model.evaluate(test_ds, return_dict=True, verbose=1)\n",
        "        metrics_dict['test_accuracy'] = test_metrics['accuracy']\n",
        "        metrics_dict['test_f1_score'] = test_metrics['f1_score']\n",
        "        metrics_dict['test_loss'] = test_metrics['loss']\n",
        "\n",
        "        print(f\"\\nTest metrics:\")\n",
        "        print(f\"- Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "        print(f\"- F1 Score: {test_metrics['f1_score']:.4f}\")\n",
        "        print(f\"- Loss: {test_metrics['loss']:.4f}\")\n",
        "\n",
        "    # Save metrics to CSV\n",
        "    metrics_df.to_csv(f'{results_dir}/best_model_metrics.csv', index=False)\n",
        "\n",
        "    return best_hp, best_model, tuner, metrics_df\n",
        "\n",
        "def compare_all_models(train_ds, val_ds, test_ds, num_classes=400, max_epochs=30):\n",
        "    \"\"\"\n",
        "    Run Hyperband tuning for all models and compare results\n",
        "    \"\"\"\n",
        "    model_types = ['resnet', 'mobilenet', 'vit', 'efficientnet', 'densenet']\n",
        "    all_results = {}\n",
        "\n",
        "    # Run tuning for each model type\n",
        "    for model_type in model_types:\n",
        "        print(f\"\\n{'-'*80}\")\n",
        "        print(f\"TUNING {model_type.upper()}\")\n",
        "        print(f\"{'-'*80}\")\n",
        "\n",
        "        best_hp, best_model, tuner, trials_df = run_hyperband_tuning(\n",
        "            train_ds=train_ds,\n",
        "            val_ds=val_ds,\n",
        "            model_type=model_type,\n",
        "            max_epochs=max_epochs,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_results = best_model.evaluate(test_ds, return_dict=True)\n",
        "\n",
        "        # Store results\n",
        "        all_results[model_type] = {\n",
        "            'hyperparameters': best_hp.values,\n",
        "            'model': best_model,\n",
        "            'tuner': tuner,\n",
        "            'trials_df': trials_df,\n",
        "            'test_metrics': test_results\n",
        "        }\n",
        "\n",
        "    # Create comparison table\n",
        "    comparison_data = []\n",
        "    for model_type, result in all_results.items():\n",
        "        # Get the best trial\n",
        "        best_trial = result['trials_df'].sort_values('overfitting').iloc[0]\n",
        "\n",
        "        comparison_data.append({\n",
        "            'Model': model_type,\n",
        "            'Val F1': best_trial['val_f1'],\n",
        "            'Train F1': best_trial['train_f1'],\n",
        "            'Val Acc': best_trial['val_accuracy'],\n",
        "            'Train Acc': best_trial['train_accuracy'],\n",
        "            'Val Loss': best_trial['val_loss'],\n",
        "            'Train Loss': best_trial['train_loss'],\n",
        "            'Overfitting': best_trial['overfitting'],\n",
        "            'Time (s)': best_trial['time'],\n",
        "            'Test Acc': result['test_metrics']['accuracy'],\n",
        "            'Test F1': result['test_metrics']['f1_score']\n",
        "        })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "    # Sort by overfitting (lower is better)\n",
        "    comparison_df = comparison_df.sort_values('Overfitting')\n",
        "\n",
        "    print(\"\\nModel Comparison (sorted by least overfitting):\")\n",
        "    print(comparison_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
        "\n",
        "    # Save comparison to CSV\n",
        "    comparison_df.to_csv('model_comparison.csv', index=False)\n",
        "\n",
        "    # Find the model with least overfitting\n",
        "    best_model_type = comparison_df.iloc[0]['Model']\n",
        "    print(f\"\\nModel with least overfitting: {best_model_type.upper()}\")\n",
        "\n",
        "    return all_results, comparison_df"
      ],
      "metadata": {
        "id": "JGvyP1hQZoWw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model Training and Evaluation"
      ],
      "metadata": {
        "id": "uzCTx-3aEsku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the ultra-simple hyperband tuning\n",
        "best_hp, best_model, tuner, metrics_df = run_hyperband_tuning(\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    test_ds=test_ds,  # Optional, pass None if not ready to evaluate on test set\n",
        "    model_type='densenet',  # Change to any model type\n",
        "    max_epochs=30,\n",
        "    num_classes=202\n",
        ")\n",
        "\n",
        "# After tuning, train the best model for more epochs if desired\n",
        "batch_size = best_hp.values.get('batch_size', 32)\n",
        "print(f\"\\nTraining the best model with batch size {batch_size} for 60 epochs...\")\n",
        "\n",
        "# Train for more epochs\n",
        "history = best_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=60,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            f'best_{model_type}_model.keras',\n",
        "            save_best_only=True,\n",
        "            monitor='val_loss'\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{model_type}_training_history.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b3eggCgadLR",
        "outputId": "96fd4b3d-30a4-4d09-db79-5f522330d60f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 Complete [00h 02m 10s]\n",
            "val_accuracy: 0.05865921825170517\n",
            "\n",
            "Best val_accuracy So Far: 0.20111732184886932\n",
            "Total elapsed time: 00h 18m 55s\n",
            "\n",
            "Search: Running Trial #4\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "simple            |complex           |augmentation_type\n",
            "8                 |7                 |unfrozen_layers\n",
            "169               |169               |densenet_variant\n",
            "8                 |8                 |batch_size\n",
            "0.00019349        |0.0011555         |learning_rate\n",
            "sgd               |adam              |optimizer\n",
            "0.95              |0.85              |momentum\n",
            "0.00012173        |0.00012223        |weight_decay\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "3                 |3                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "Unfrozen 8 of 707 layers in DenseNet169\n",
            "Epoch 1/2\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2s/step - accuracy: 0.0095 - auc: 0.5070 - f1_score: 0.0040 - loss: 5.7493 - val_accuracy: 0.0056 - val_auc: 0.5229 - val_f1_score: 0.0026 - val_loss: 5.5464\n",
            "Epoch 2/2\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 2s/step - accuracy: 0.0071 - auc: 0.5254 - f1_score: 0.0026 - loss: 5.6282 - val_accuracy: 0.0223 - val_auc: 0.5707 - val_f1_score: 0.0047 - val_loss: 5.3483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l-FFdf-OcYSc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
