{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PandaReach-v3 Visual Training\n",
    "\n",
    "This notebook runs a training session with visual rendering, so you can see the robot moving as it learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "# Import project modules\n",
    "from sac import SAC\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create environment with rendering enabled\n",
    "env_name = \"PandaReach-v3\"\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "eval_env = gym.make(env_name)\n",
    "\n",
    "# Print environment information\n",
    "print(\"Environment Information:\")\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "print(f\"Action Space High: {env.action_space.high}\")\n",
    "print(f\"Action Space Low: {env.action_space.low}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to evaluate a policy\n",
    "def evaluate_policy(policy, env, eval_episodes=5):\n",
    "    \"\"\"Evaluate the policy without rendering\"\"\"\n",
    "    avg_reward = 0.\n",
    "    successes = 0\n",
    "    \n",
    "    for _ in range(eval_episodes):\n",
    "        obs_dict, _ = env.reset()\n",
    "        obs = obs_dict['observation']\n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            action = policy.select_action(np.array(obs), noise=0)\n",
    "            obs_dict, reward, done, truncated, info = env.step(action)\n",
    "            obs = obs_dict['observation']\n",
    "            avg_reward += reward\n",
    "            \n",
    "            if 'is_success' in info and info['is_success'] == 1.0:\n",
    "                successes += 1\n",
    "                break\n",
    "    \n",
    "    avg_reward /= eval_episodes\n",
    "    success_rate = successes / eval_episodes\n",
    "    \n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}, Success rate: {success_rate:.3f}\")\n",
    "    return avg_reward, success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up the agent\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Get state and action dimensions\n",
    "state_dim = env.observation_space['observation'].shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "# Initialize policy\n",
    "policy = SAC(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    action_space=env.action_space,\n",
    "    device=device,\n",
    "    hidden_dim=256,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    alpha=0.2,\n",
    "    automatic_entropy_tuning=True\n",
    ")\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = ReplayBuffer(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    max_size=1000000,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visual training loop - watch the robot learn!\n",
    "max_episodes = 1000\n",
    "max_steps_per_episode = 50\n",
    "\n",
    "# For collecting statistics\n",
    "all_rewards = []\n",
    "all_lengths = []\n",
    "success_rates = []\n",
    "eval_interval = 10  # Evaluate every 10 episodes\n",
    "\n",
    "for episode in range(1, max_episodes + 1):\n",
    "    print(f\"\\nEpisode {episode}/{max_episodes}\")\n",
    "    \n",
    "    # Reset environment\n",
    "    state_dict, _ = env.reset(seed=seed+episode)\n",
    "    state = state_dict['observation']\n",
    "    \n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    \n",
    "    # Run one episode\n",
    "    while not (done or truncated) and episode_steps < max_steps_per_episode:\n",
    "        # Select action\n",
    "        action = policy.select_action(state)\n",
    "        \n",
    "        # Execute action\n",
    "        next_state_dict, reward, done, truncated, info = env.step(action)\n",
    "        next_state = next_state_dict['observation']\n",
    "        \n",
    "        # Save in replay buffer\n",
    "        replay_buffer.add(state, action, next_state, reward, done)\n",
    "        \n",
    "        # Update state and counters\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        episode_steps += 1\n",
    "        \n",
    "        # Train agent (learning happens here)\n",
    "        policy.update_parameters(replay_buffer)\n",
    "        \n",
    "        # Check for success\n",
    "        if 'is_success' in info and info['is_success'] == 1.0:\n",
    "            print(f\"Success! Goal reached at step {episode_steps}\")\n",
    "            break\n",
    "    \n",
    "    # End of episode stats\n",
    "    print(f\"Episode {episode}: Reward = {episode_reward:.3f}, Steps = {episode_steps}\")\n",
    "    all_rewards.append(episode_reward)\n",
    "    all_lengths.append(episode_steps)\n",
    "    \n",
    "    # Evaluate periodically\n",
    "    if episode % eval_interval == 0:\n",
    "        avg_reward, success_rate = evaluate_policy(policy, eval_env)\n",
    "        success_rates.append(success_rate)\n",
    "        \n",
    "        # Plot progress\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot rewards\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(all_rewards)\n",
    "        plt.title('Episode Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        \n",
    "        # Plot lengths\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(all_lengths)\n",
    "        plt.title('Episode Lengths')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps')\n",
    "        \n",
    "        # Plot success rates\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(np.arange(eval_interval, episode+1, eval_interval), success_rates)\n",
    "        plt.title('Success Rate')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Success Rate')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        if success_rate >= 0.8:  # Save when we reach 80% success\n",
    "            print(\"Saving model checkpoint - good performance!\")\n",
    "            policy.save(f\"./results/sac_checkpoint_episode_{episode}\")\n",
    "\n",
    "# Close environment when done\n",
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the final policy\n",
    "test_env = gym.make(env_name, render_mode=\"human\")\n",
    "test_episodes = 5\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    obs_dict, _ = test_env.reset()\n",
    "    obs = obs_dict['observation']\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    print(f\"\\nTesting Episode {episode+1}/{test_episodes}\")\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        # Choose action without exploration noise\n",
    "        action = policy.select_action(obs, noise=0)\n",
    "        \n",
    "        # Execute action\n",
    "        obs_dict, reward, done, truncated, info = test_env.step(action)\n",
    "        obs = obs_dict['observation']\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        # Check for success\n",
    "        if 'is_success' in info and info['is_success'] == 1.0:\n",
    "            print(f\"Success! Goal reached at step {steps}\")\n",
    "            time.sleep(1)  # Pause for a moment to see the success\n",
    "            break\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Reward = {episode_reward:.3f}, Steps = {steps}\")\n",
    "\n",
    "test_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
